{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#0. Data preparation"
      ],
      "metadata": {
        "id": "2_gf5DC68wGe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7Dneiox4naQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from torchvision.datasets import CIFAR10"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = './datasets/cifar10'\n",
        "train_path = os.path.join(dataset_path, 'train')\n",
        "val_path = os.path.join(dataset_path, 'val')"
      ],
      "metadata": {
        "id": "c4ofgVpFfKQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(train_path, exist_ok=True)\n",
        "os.makedirs(val_path, exist_ok=True)"
      ],
      "metadata": {
        "id": "JiPyBXw3fVO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor()])"
      ],
      "metadata": {
        "id": "kMRYb-gyfVL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = CIFAR10(root=train_path, train=True, download=True, transform=transform)\n",
        "valset = CIFAR10(root=val_path, train=False, download=True, transform=transform)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yi4RsLXifVJO",
        "outputId": "3ec127e3-cd01-49fb-e801-8ff2b21b7837"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./datasets/cifar10/train/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:03<00:00, 44309987.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./datasets/cifar10/train/cifar-10-python.tar.gz to ./datasets/cifar10/train\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./datasets/cifar10/val/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:05<00:00, 31137805.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./datasets/cifar10/val/cifar-10-python.tar.gz to ./datasets/cifar10/val\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_images(dataset, root_path):\n",
        "    to_pil = transforms.ToPILImage()\n",
        "    for idx, (image, label) in enumerate(dataset):\n",
        "        class_path = os.path.join(root_path, str(label))\n",
        "        os.makedirs(class_path, exist_ok=True)\n",
        "        image_path = os.path.join(class_path, f'{idx}.png')\n",
        "        pil_image = to_pil(image)\n",
        "        pil_image.save(image_path)"
      ],
      "metadata": {
        "id": "t11pJYe-fVDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_images(trainset, train_path)\n",
        "save_images(valset, val_path)"
      ],
      "metadata": {
        "id": "V8anxYBhgApg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Models"
      ],
      "metadata": {
        "id": "QKMoRSwK85zs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/gosh-a/solo-learn.git\n",
        "%cd solo-learn\n",
        "!pip install ."
      ],
      "metadata": {
        "id": "FyFmfPkoJcF3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "24ff3f99-c2a6-4685-dcc9-92971c81421d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'solo-learn'...\n",
            "remote: Enumerating objects: 5161, done.\u001b[K\n",
            "remote: Counting objects: 100% (294/294), done.\u001b[K\n",
            "remote: Compressing objects: 100% (177/177), done.\u001b[K\n",
            "remote: Total 5161 (delta 176), reused 191 (delta 115), pack-reused 4867\u001b[K\n",
            "Receiving objects: 100% (5161/5161), 5.18 MiB | 10.11 MiB/s, done.\n",
            "Resolving deltas: 100% (3637/3637), done.\n",
            "/content/solo-learn\n",
            "Processing /content/solo-learn\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from solo-learn==1.0.6) (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from solo-learn==1.0.6) (0.18.0+cu121)\n",
            "Collecting einops (from solo-learn==1.0.6)\n",
            "  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lightning==2.1.2 (from solo-learn==1.0.6)\n",
            "  Downloading lightning-2.1.2-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchmetrics<0.12.0,>=0.6.0 (from solo-learn==1.0.6)\n",
            "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from solo-learn==1.0.6) (4.66.4)\n",
            "Collecting wandb (from solo-learn==1.0.6)\n",
            "  Downloading wandb-0.17.2-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from solo-learn==1.0.6) (1.11.4)\n",
            "Collecting timm (from solo-learn==1.0.6)\n",
            "  Downloading timm-1.0.7-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from solo-learn==1.0.6) (1.2.2)\n",
            "Collecting hydra-core (from solo-learn==1.0.6)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning==2.1.2->solo-learn==1.0.6) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]<2025.0,>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.1.2->solo-learn==1.0.6) (2023.6.0)\n",
            "Collecting lightning-utilities<2.0,>=0.8.0 (from lightning==2.1.2->solo-learn==1.0.6)\n",
            "  Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from lightning==2.1.2->solo-learn==1.0.6) (1.25.2)\n",
            "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.1.2->solo-learn==1.0.6) (24.1)\n",
            "Requirement already satisfied: typing-extensions<6.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.1.2->solo-learn==1.0.6) (4.12.2)\n",
            "Collecting pytorch-lightning (from lightning==2.1.2->solo-learn==1.0.6)\n",
            "  Downloading pytorch_lightning-2.3.0-py3-none-any.whl (812 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.2/812.2 kB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->solo-learn==1.0.6) (3.15.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->solo-learn==1.0.6) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->solo-learn==1.0.6) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->solo-learn==1.0.6) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->solo-learn==1.0.6)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->solo-learn==1.0.6)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->solo-learn==1.0.6)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->solo-learn==1.0.6)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->solo-learn==1.0.6)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->solo-learn==1.0.6)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->solo-learn==1.0.6)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->solo-learn==1.0.6)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->solo-learn==1.0.6)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.10.0->solo-learn==1.0.6)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->solo-learn==1.0.6)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->solo-learn==1.0.6) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->solo-learn==1.0.6)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.11.1->solo-learn==1.0.6) (9.4.0)\n",
            "Collecting omegaconf<2.4,>=2.2 (from hydra-core->solo-learn==1.0.6)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting antlr4-python3-runtime==4.9.* (from hydra-core->solo-learn==1.0.6)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->solo-learn==1.0.6) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->solo-learn==1.0.6) (3.5.0)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm->solo-learn==1.0.6) (0.23.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm->solo-learn==1.0.6) (0.4.3)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb->solo-learn==1.0.6) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb->solo-learn==1.0.6)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb->solo-learn==1.0.6)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb->solo-learn==1.0.6) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb->solo-learn==1.0.6) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->solo-learn==1.0.6) (5.9.5)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->solo-learn==1.0.6) (2.31.0)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb->solo-learn==1.0.6)\n",
            "  Downloading sentry_sdk-2.6.0-py2.py3-none-any.whl (296 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.1/296.1 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setproctitle (from wandb->solo-learn==1.0.6)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb->solo-learn==1.0.6) (67.7.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb->solo-learn==1.0.6) (1.16.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2025.0,>2021.06.0->lightning==2.1.2->solo-learn==1.0.6) (3.9.5)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb->solo-learn==1.0.6)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb->solo-learn==1.0.6) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb->solo-learn==1.0.6) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb->solo-learn==1.0.6) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb->solo-learn==1.0.6) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->solo-learn==1.0.6) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->solo-learn==1.0.6) (1.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning==2.1.2->solo-learn==1.0.6) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning==2.1.2->solo-learn==1.0.6) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning==2.1.2->solo-learn==1.0.6) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning==2.1.2->solo-learn==1.0.6) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning==2.1.2->solo-learn==1.0.6) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning==2.1.2->solo-learn==1.0.6) (4.0.3)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->solo-learn==1.0.6)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: solo-learn, antlr4-python3-runtime\n",
            "  Building wheel for solo-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for solo-learn: filename=solo_learn-1.0.6-py3-none-any.whl size=246111 sha256=89f6255f57a814f99d9f6fe11bc8d235a0ba77c8dec92581193cd27426f9bd54\n",
            "  Stored in directory: /root/.cache/pip/wheels/93/3e/f4/4aaca42798fee7df65f21d8ed5d62a4c498e4f325b9bcc120f\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=eb2988682f4690c99daad5c0cbf6b7e487bcc868c435e38dbad7333b4737e210\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built solo-learn antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, smmap, setproctitle, sentry-sdk, omegaconf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, einops, docker-pycreds, nvidia-cusparse-cu12, nvidia-cudnn-cu12, hydra-core, gitdb, nvidia-cusolver-cu12, gitpython, wandb, torchmetrics, timm, pytorch-lightning, lightning, solo-learn\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 docker-pycreds-0.4.0 einops-0.8.0 gitdb-4.0.11 gitpython-3.1.43 hydra-core-1.3.2 lightning-2.1.2 lightning-utilities-0.11.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 omegaconf-2.3.0 pytorch-lightning-2.3.0 sentry-sdk-2.6.0 setproctitle-1.3.3 smmap-5.0.1 solo-learn-1.0.6 timm-1.0.7 torchmetrics-0.11.4 wandb-0.17.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              },
              "id": "6979c12cb07b418a92d9fffd03460064"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtpo9m1bAJvY",
        "outputId": "d0080f85-74e5-4458-a59b-0147bd2ab3a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Jun 22 22:03:58 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simclr"
      ],
      "metadata": {
        "id": "k5E2b2KAe5Kw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### pretrain the model"
      ],
      "metadata": {
        "id": "x-PCJjQWClRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "command = ['python3', 'solo-learn/main_pretrain.py',\n",
        "           '--config-path', 'scripts/pretrain/cifar/',\n",
        "           '--config-name', 'simclr.yaml',\n",
        "           '++wandb.offline=True']"
      ],
      "metadata": {
        "id": "vyk8DB-C31lJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "\n",
        "for line in process.stdout:\n",
        "    print(line, end='')\n",
        "\n",
        "for line in process.stderr:\n",
        "    print(line, end='')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pi_SjLbv3_2C",
        "outputId": "32cc3b89-cd03-4129-d89d-f7c57323a39a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "\n",
            "Training: |          | 0/? [00:00<?, ?it/s]\n",
            "Training:   0%|          | 0/195 [00:00<?, ?it/s]\n",
            "Epoch 0:   0%|          | 0/195 [00:00<?, ?it/s] \n",
            "Epoch 0:  10%|█         | 20/195 [00:14<02:08,  1.36it/s]\n",
            "Epoch 0:  10%|█         | 20/195 [00:14<02:08,  1.36it/s, v_num=eczv]\n",
            "Epoch 0:  21%|██        | 40/195 [00:27<01:45,  1.47it/s, v_num=eczv]\n",
            "Epoch 0:  21%|██        | 40/195 [00:27<01:45,  1.47it/s, v_num=eczv]\n",
            "Epoch 0:  31%|███       | 60/195 [00:37<01:24,  1.61it/s, v_num=eczv]\n",
            "Epoch 0:  31%|███       | 60/195 [00:37<01:24,  1.61it/s, v_num=eczv]\n",
            "Epoch 0:  41%|████      | 80/195 [00:48<01:09,  1.66it/s, v_num=eczv]\n",
            "Epoch 0:  41%|████      | 80/195 [00:48<01:09,  1.66it/s, v_num=eczv]\n",
            "Epoch 0:  51%|█████▏    | 100/195 [01:00<00:57,  1.64it/s, v_num=eczv]\n",
            "Epoch 0:  51%|█████▏    | 100/195 [01:00<00:57,  1.64it/s, v_num=eczv]\n",
            "Epoch 0:  62%|██████▏   | 120/195 [01:11<00:44,  1.69it/s, v_num=eczv]\n",
            "Epoch 0:  62%|██████▏   | 120/195 [01:11<00:44,  1.69it/s, v_num=eczv]\n",
            "Epoch 0:  72%|███████▏  | 140/195 [01:21<00:32,  1.71it/s, v_num=eczv]\n",
            "Epoch 0:  72%|███████▏  | 140/195 [01:21<00:32,  1.71it/s, v_num=eczv]\n",
            "Epoch 0:  82%|████████▏ | 160/195 [01:33<00:20,  1.71it/s, v_num=eczv]\n",
            "Epoch 0:  82%|████████▏ | 160/195 [01:33<00:20,  1.71it/s, v_num=eczv]\n",
            "Epoch 0:  92%|█████████▏| 180/195 [01:44<00:08,  1.72it/s, v_num=eczv]\n",
            "Epoch 0:  92%|█████████▏| 180/195 [01:44<00:08,  1.72it/s, v_num=eczv]\n",
            "Epoch 0: 100%|██████████| 195/195 [01:52<00:00,  1.74it/s, v_num=eczv]\n",
            "Epoch 0: 100%|██████████| 195/195 [01:52<00:00,  1.74it/s, v_num=eczv]\n",
            "Epoch 0: 100%|██████████| 195/195 [01:52<00:00,  1.73it/s, v_num=eczv]\n",
            "Epoch 0:   0%|          | 0/195 [00:00<?, ?it/s, v_num=eczv]          \n",
            "Epoch 1:   0%|          | 0/195 [00:00<?, ?it/s, v_num=eczv]\n",
            "Epoch 1:  10%|█         | 20/195 [00:13<02:01,  1.44it/s, v_num=eczv]\n",
            "Epoch 1:  10%|█         | 20/195 [00:13<02:01,  1.44it/s, v_num=eczv]\n",
            "Epoch 1:  21%|██        | 40/195 [00:24<01:35,  1.62it/s, v_num=eczv]\n",
            "Epoch 1:  21%|██        | 40/195 [00:24<01:35,  1.62it/s, v_num=eczv]\n",
            "Epoch 1:  31%|███       | 60/195 [00:35<01:20,  1.69it/s, v_num=eczv]\n",
            "Epoch 1:  31%|███       | 60/195 [00:35<01:20,  1.69it/s, v_num=eczv]\n",
            "Epoch 1:  41%|████      | 80/195 [00:48<01:09,  1.67it/s, v_num=eczv]\n",
            "Epoch 1:  41%|████      | 80/195 [00:48<01:09,  1.67it/s, v_num=eczv]\n",
            "Epoch 1:  51%|█████▏    | 100/195 [00:58<00:55,  1.70it/s, v_num=eczv]\n",
            "Epoch 1:  51%|█████▏    | 100/195 [00:58<00:55,  1.70it/s, v_num=eczv]\n",
            "Epoch 1:  62%|██████▏   | 120/195 [01:09<00:43,  1.73it/s, v_num=eczv]\n",
            "Epoch 1:  62%|██████▏   | 120/195 [01:09<00:43,  1.73it/s, v_num=eczv]\n",
            "Epoch 1:  72%|███████▏  | 140/195 [01:21<00:31,  1.72it/s, v_num=eczv]\n",
            "Epoch 1:  72%|███████▏  | 140/195 [01:21<00:31,  1.72it/s, v_num=eczv]\n",
            "Epoch 1:  82%|████████▏ | 160/195 [01:33<00:20,  1.71it/s, v_num=eczv]\n",
            "Epoch 1:  82%|████████▏ | 160/195 [01:33<00:20,  1.71it/s, v_num=eczv]\n",
            "Epoch 1:  92%|█████████▏| 180/195 [01:43<00:08,  1.74it/s, v_num=eczv]\n",
            "Epoch 1:  92%|█████████▏| 180/195 [01:43<00:08,  1.74it/s, v_num=eczv]\n",
            "Epoch 1: 100%|██████████| 195/195 [01:52<00:00,  1.74it/s, v_num=eczv]\n",
            "Epoch 1: 100%|██████████| 195/195 [01:52<00:00,  1.74it/s, v_num=eczv]\n",
            "Epoch 1: 100%|██████████| 195/195 [01:52<00:00,  1.73it/s, v_num=eczv]\n",
            "Epoch 1:   0%|          | 0/195 [00:00<?, ?it/s, v_num=eczv]          \n",
            "Epoch 2:   0%|          | 0/195 [00:00<?, ?it/s, v_num=eczv]\n",
            "Epoch 2:  10%|█         | 20/195 [00:13<01:57,  1.49it/s, v_num=eczv]\n",
            "Epoch 2:  10%|█         | 20/195 [00:13<01:57,  1.49it/s, v_num=eczv]\n",
            "Epoch 2:  21%|██        | 40/195 [00:23<01:32,  1.68it/s, v_num=eczv]\n",
            "Epoch 2:  21%|██        | 40/195 [00:23<01:32,  1.68it/s, v_num=eczv]\n",
            "Epoch 2:  31%|███       | 60/195 [00:36<01:21,  1.66it/s, v_num=eczv]\n",
            "Epoch 2:  31%|███       | 60/195 [00:36<01:21,  1.66it/s, v_num=eczv]\n",
            "Epoch 2:  41%|████      | 80/195 [00:47<01:08,  1.68it/s, v_num=eczv]\n",
            "Epoch 2:  41%|████      | 80/195 [00:47<01:08,  1.68it/s, v_num=eczv]\n",
            "Epoch 2:  51%|█████▏    | 100/195 [00:57<00:54,  1.73it/s, v_num=eczv]\n",
            "Epoch 2:  51%|█████▏    | 100/195 [00:57<00:54,  1.73it/s, v_num=eczv]\n",
            "Epoch 2:  62%|██████▏   | 120/195 [01:10<00:43,  1.71it/s, v_num=eczv]\n",
            "Epoch 2:  62%|██████▏   | 120/195 [01:10<00:43,  1.71it/s, v_num=eczv]\n",
            "Epoch 2:  72%|███████▏  | 140/195 [01:22<00:32,  1.70it/s, v_num=eczv]\n",
            "Epoch 2:  72%|███████▏  | 140/195 [01:22<00:32,  1.70it/s, v_num=eczv]\n",
            "Epoch 2:  82%|████████▏ | 160/195 [01:31<00:20,  1.75it/s, v_num=eczv]\n",
            "Epoch 2:  82%|████████▏ | 160/195 [01:31<00:20,  1.75it/s, v_num=eczv]\n",
            "Epoch 2:  92%|█████████▏| 180/195 [01:43<00:08,  1.73it/s, v_num=eczv]\n",
            "Epoch 2:  92%|█████████▏| 180/195 [01:43<00:08,  1.73it/s, v_num=eczv]\n",
            "Epoch 2: 100%|██████████| 195/195 [01:51<00:00,  1.75it/s, v_num=eczv]\n",
            "Epoch 2: 100%|██████████| 195/195 [01:51<00:00,  1.75it/s, v_num=eczv]\n",
            "Epoch 2: 100%|██████████| 195/195 [01:52<00:00,  1.74it/s, v_num=eczv]\n",
            "Epoch 2:   0%|          | 0/195 [00:00<?, ?it/s, v_num=eczv]          \n",
            "Epoch 3:   0%|          | 0/195 [00:00<?, ?it/s, v_num=eczv]\n",
            "Epoch 3:  10%|█         | 20/195 [00:11<01:40,  1.75it/s, v_num=eczv]\n",
            "Epoch 3:  10%|█         | 20/195 [00:11<01:40,  1.75it/s, v_num=eczv]\n",
            "Epoch 3:  21%|██        | 40/195 [00:23<01:32,  1.67it/s, v_num=eczv]\n",
            "Epoch 3:  21%|██        | 40/195 [00:24<01:33,  1.67it/s, v_num=eczv]\n",
            "Epoch 3:  31%|███       | 60/195 [00:37<01:23,  1.61it/s, v_num=eczv]\n",
            "Epoch 3:  31%|███       | 60/195 [00:37<01:23,  1.61it/s, v_num=eczv]\n",
            "Epoch 3:  41%|████      | 80/195 [00:47<01:08,  1.67it/s, v_num=eczv]\n",
            "Epoch 3:  41%|████      | 80/195 [00:47<01:08,  1.67it/s, v_num=eczv]\n",
            "Epoch 3:  51%|█████▏    | 100/195 [00:59<00:56,  1.69it/s, v_num=eczv]\n",
            "Epoch 3:  51%|█████▏    | 100/195 [00:59<00:56,  1.69it/s, v_num=eczv]\n",
            "Epoch 3:  62%|██████▏   | 120/195 [01:11<00:44,  1.68it/s, v_num=eczv]\n",
            "Epoch 3:  62%|██████▏   | 120/195 [01:11<00:44,  1.68it/s, v_num=eczv]\n",
            "Epoch 3:  72%|███████▏  | 140/195 [01:22<00:32,  1.71it/s, v_num=eczv]\n",
            "Epoch 3:  72%|███████▏  | 140/195 [01:22<00:32,  1.71it/s, v_num=eczv]\n",
            "Epoch 3:  82%|████████▏ | 160/195 [01:33<00:20,  1.72it/s, v_num=eczv]\n",
            "Epoch 3:  82%|████████▏ | 160/195 [01:33<00:20,  1.72it/s, v_num=eczv]\n",
            "Epoch 3:  92%|█████████▏| 180/195 [01:44<00:08,  1.72it/s, v_num=eczv]\n",
            "Epoch 3:  92%|█████████▏| 180/195 [01:44<00:08,  1.72it/s, v_num=eczv]\n",
            "Epoch 3: 100%|██████████| 195/195 [01:51<00:00,  1.76it/s, v_num=eczv]\n",
            "Epoch 3: 100%|██████████| 195/195 [01:51<00:00,  1.76it/s, v_num=eczv]\n",
            "Epoch 3: 100%|██████████| 195/195 [01:51<00:00,  1.75it/s, v_num=eczv]\n",
            "Epoch 3:   0%|          | 0/195 [00:00<?, ?it/s, v_num=eczv]          \n",
            "Epoch 4:   0%|          | 0/195 [00:00<?, ?it/s, v_num=eczv]\n",
            "Epoch 4:  10%|█         | 20/195 [00:13<01:57,  1.49it/s, v_num=eczv]\n",
            "Epoch 4:  10%|█         | 20/195 [00:13<01:57,  1.49it/s, v_num=eczv]\n",
            "Epoch 4:  21%|██        | 40/195 [00:25<01:38,  1.57it/s, v_num=eczv]\n",
            "Epoch 4:  21%|██        | 40/195 [00:25<01:38,  1.57it/s, v_num=eczv]\n",
            "Epoch 4:  31%|███       | 60/195 [00:36<01:22,  1.63it/s, v_num=eczv]\n",
            "Epoch 4:  31%|███       | 60/195 [00:36<01:22,  1.63it/s, v_num=eczv]\n",
            "Epoch 4:  41%|████      | 80/195 [00:47<01:08,  1.67it/s, v_num=eczv]\n",
            "Epoch 4:  41%|████      | 80/195 [00:47<01:08,  1.67it/s, v_num=eczv]\n",
            "Epoch 4:  51%|█████▏    | 100/195 [01:00<00:57,  1.66it/s, v_num=eczv]\n",
            "Epoch 4:  51%|█████▏    | 100/195 [01:00<00:57,  1.66it/s, v_num=eczv]\n",
            "Epoch 4:  62%|██████▏   | 120/195 [01:12<00:45,  1.64it/s, v_num=eczv]\n",
            "Epoch 4:  62%|██████▏   | 120/195 [01:12<00:45,  1.64it/s, v_num=eczv]\n",
            "Epoch 4:  72%|███████▏  | 140/195 [01:22<00:32,  1.70it/s, v_num=eczv]\n",
            "Epoch 4:  72%|███████▏  | 140/195 [01:22<00:32,  1.70it/s, v_num=eczv]\n",
            "Epoch 4:  82%|████████▏ | 160/195 [01:35<00:20,  1.68it/s, v_num=eczv]\n",
            "Epoch 4:  82%|████████▏ | 160/195 [01:35<00:20,  1.68it/s, v_num=eczv]\n",
            "Epoch 4:  92%|█████████▏| 180/195 [01:49<00:09,  1.65it/s, v_num=eczv]\n",
            "Epoch 4:  92%|█████████▏| 180/195 [01:49<00:09,  1.65it/s, v_num=eczv]\n",
            "Epoch 4: 100%|██████████| 195/195 [01:55<00:00,  1.69it/s, v_num=eczv]\n",
            "Epoch 4: 100%|██████████| 195/195 [01:55<00:00,  1.69it/s, v_num=eczv]\n",
            "Epoch 4: 100%|██████████| 195/195 [01:56<00:00,  1.68it/s, v_num=eczv]\n",
            "Epoch 4: 100%|██████████| 195/195 [01:56<00:00,  1.68it/s, v_num=eczv]\n",
            "/content/solo-learn/solo/backbones/convnext/convnext.py:27: UserWarning: Overwriting convnext_tiny in registry with solo.backbones.convnext.convnext.convnext_tiny. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def convnext_tiny(**kwargs):\n",
            "/content/solo-learn/solo/backbones/convnext/convnext.py:34: UserWarning: Overwriting convnext_small in registry with solo.backbones.convnext.convnext.convnext_small. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def convnext_small(**kwargs):\n",
            "/content/solo-learn/solo/backbones/convnext/convnext.py:41: UserWarning: Overwriting convnext_base in registry with solo.backbones.convnext.convnext.convnext_base. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def convnext_base(**kwargs):\n",
            "/content/solo-learn/solo/backbones/convnext/convnext.py:48: UserWarning: Overwriting convnext_large in registry with solo.backbones.convnext.convnext.convnext_large. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def convnext_large(**kwargs):\n",
            "/content/solo-learn/solo/backbones/poolformer/poolformer.py:422: UserWarning: Overwriting poolformer_s12 in registry with solo.backbones.poolformer.poolformer.poolformer_s12. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def poolformer_s12(**kwargs):\n",
            "/content/solo-learn/solo/backbones/poolformer/poolformer.py:447: UserWarning: Overwriting poolformer_s24 in registry with solo.backbones.poolformer.poolformer.poolformer_s24. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def poolformer_s24(**kwargs):\n",
            "/content/solo-learn/solo/backbones/poolformer/poolformer.py:468: UserWarning: Overwriting poolformer_s36 in registry with solo.backbones.poolformer.poolformer.poolformer_s36. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def poolformer_s36(**kwargs):\n",
            "/content/solo-learn/solo/backbones/poolformer/poolformer.py:490: UserWarning: Overwriting poolformer_m36 in registry with solo.backbones.poolformer.poolformer.poolformer_m36. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def poolformer_m36(**kwargs):\n",
            "/content/solo-learn/solo/backbones/poolformer/poolformer.py:512: UserWarning: Overwriting poolformer_m48 in registry with solo.backbones.poolformer.poolformer.poolformer_m48. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def poolformer_m48(**kwargs):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "wandb: Tracking run with wandb version 0.17.2\n",
            "wandb: W&B syncing is set to `offline` in this directory.  \n",
            "wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.\n",
            "wandb: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
            "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "wandb: \n",
            "wandb: Run history:\n",
            "wandb:                  epoch ▁▁▁▁▃▃▃▃▃▅▅▅▅▅▆▆▆▆▆█████\n",
            "wandb:       lr-LARS/backbone ▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██\n",
            "wandb:     lr-LARS/classifier ▁▁▂▂▃▃▃▄▄▄▅▅▆▆▆▇▇██\n",
            "wandb:      lr-LARS/projector ▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██\n",
            "wandb:       train_acc1_epoch ▁▄▅▇█\n",
            "wandb:        train_acc1_step ▁▃▃▄▃▄▅▄▆▅▅▅▅▆▇▆▇▆█\n",
            "wandb:       train_acc5_epoch ▁▅▆██\n",
            "wandb:        train_acc5_step ▁▅▄▆▅▆▆▆▇▆▇▇███▇▇██\n",
            "wandb: train_class_loss_epoch █▅▃▂▁\n",
            "wandb:  train_class_loss_step █▇▆▆▆▅▅▅▃▅▃▄▃▄▂▃▃▃▁\n",
            "wandb:   train_nce_loss_epoch █▅▃▂▁\n",
            "wandb:    train_nce_loss_step █▇▇▆▆▄▅▃▃▄▂▃▂▂▁▂▁▂▁\n",
            "wandb:    trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██\n",
            "wandb: \n",
            "wandb: Run summary:\n",
            "wandb:                  epoch 4\n",
            "wandb:       lr-LARS/backbone 0.19478\n",
            "wandb:     lr-LARS/classifier 0.04871\n",
            "wandb:      lr-LARS/projector 0.19478\n",
            "wandb:       train_acc1_epoch 38.78405\n",
            "wandb:        train_acc1_step 45.3125\n",
            "wandb:       train_acc5_epoch 86.61058\n",
            "wandb:        train_acc5_step 87.10938\n",
            "wandb: train_class_loss_epoch 1.74894\n",
            "wandb:  train_class_loss_step 1.59421\n",
            "wandb:   train_nce_loss_epoch 4.37928\n",
            "wandb:    train_nce_loss_step 4.28609\n",
            "wandb:    trainer/global_step 974\n",
            "wandb: \n",
            "wandb: You can sync this run to the cloud by running:\n",
            "wandb: wandb sync ./wandb/offline-run-20240622_220435-1vzreczv\n",
            "wandb: Find logs at: ./wandb/offline-run-20240622_220435-1vzreczv/logs\n",
            "wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### evaluate the model"
      ],
      "metadata": {
        "id": "R-3ToHMBCoRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for dirpath, _, filenames in os.walk(os.path.join(os.getcwd(), 'trained_models', 'simclr')):\n",
        "  if filenames:\n",
        "    assert filenames[1].endswith('ckpt')\n",
        "    new_path = os.path.join(dirpath, filenames[1].replace('=', '_'))\n",
        "    os.rename(os.path.join(dirpath, filenames[1]), new_path)"
      ],
      "metadata": {
        "id": "GyCb_TQGkSQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "command_path_name = '/'.join(new_path.split('/')[2:])"
      ],
      "metadata": {
        "id": "oPuUrMxQoq-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "command = ['python3', 'solo-learn/main_linear.py',\n",
        "           '--config-path', 'scripts/linear/cifar/',\n",
        "           '--config-name', 'simclr.yaml',\n",
        "           f'++pretrained_feature_extractor={command_path_name}',\n",
        "           '++wandb.offline=True']"
      ],
      "metadata": {
        "id": "bwjzVE7SuDKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "\n",
        "for line in process.stdout:\n",
        "    print(line, end='')\n",
        "\n",
        "for line in process.stderr:\n",
        "    print(line, end='')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKrhoDzkuDHZ",
        "outputId": "c0cad8ef-d8e8-414c-da56-a41d3ec50f01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Sanity Checking: |          | 0/? [00:00<?, ?it/s]\n",
            "Sanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  3.23it/s]\n",
            "                                                                           \n",
            "\n",
            "Training: |          | 0/? [00:00<?, ?it/s]\n",
            "Training:   0%|          | 0/195 [00:00<?, ?it/s]\n",
            "Epoch 0:   0%|          | 0/195 [00:00<?, ?it/s] \n",
            "Epoch 0:  10%|█         | 20/195 [00:04<00:37,  4.69it/s]\n",
            "Epoch 0:  10%|█         | 20/195 [00:04<00:37,  4.68it/s, v_num=npq2]\n",
            "Epoch 0:  21%|██        | 40/195 [00:08<00:32,  4.79it/s, v_num=npq2]\n",
            "Epoch 0:  21%|██        | 40/195 [00:08<00:32,  4.78it/s, v_num=npq2]\n",
            "Epoch 0:  31%|███       | 60/195 [00:12<00:28,  4.78it/s, v_num=npq2]\n",
            "Epoch 0:  31%|███       | 60/195 [00:12<00:28,  4.77it/s, v_num=npq2]\n",
            "Epoch 0:  41%|████      | 80/195 [00:14<00:21,  5.45it/s, v_num=npq2]\n",
            "Epoch 0:  41%|████      | 80/195 [00:14<00:21,  5.45it/s, v_num=npq2]\n",
            "Epoch 0:  51%|█████▏    | 100/195 [00:16<00:16,  5.90it/s, v_num=npq2]\n",
            "Epoch 0:  51%|█████▏    | 100/195 [00:16<00:16,  5.90it/s, v_num=npq2]\n",
            "Epoch 0:  62%|██████▏   | 120/195 [00:19<00:11,  6.31it/s, v_num=npq2]\n",
            "Epoch 0:  62%|██████▏   | 120/195 [00:19<00:11,  6.31it/s, v_num=npq2]\n",
            "Epoch 0:  72%|███████▏  | 140/195 [00:21<00:08,  6.64it/s, v_num=npq2]\n",
            "Epoch 0:  72%|███████▏  | 140/195 [00:21<00:08,  6.64it/s, v_num=npq2]\n",
            "Epoch 0:  82%|████████▏ | 160/195 [00:24<00:05,  6.64it/s, v_num=npq2]\n",
            "Epoch 0:  82%|████████▏ | 160/195 [00:24<00:05,  6.64it/s, v_num=npq2]\n",
            "Epoch 0:  92%|█████████▏| 180/195 [00:27<00:02,  6.45it/s, v_num=npq2]\n",
            "Epoch 0:  92%|█████████▏| 180/195 [00:27<00:02,  6.45it/s, v_num=npq2]\n",
            "Epoch 0: 100%|██████████| 195/195 [00:30<00:00,  6.47it/s, v_num=npq2]\n",
            "Epoch 0: 100%|██████████| 195/195 [00:30<00:00,  6.47it/s, v_num=npq2]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:  50%|█████     | 20/40 [00:01<00:01, 16.96it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 40/40 [00:02<00:00, 17.93it/s]\u001b[A\n",
            "\n",
            "                                                                        \u001b[A\n",
            "Epoch 0: 100%|██████████| 195/195 [00:33<00:00,  5.89it/s, v_num=npq2]\n",
            "Epoch 0: 100%|██████████| 195/195 [00:33<00:00,  5.87it/s, v_num=npq2]\n",
            "Epoch 0:   0%|          | 0/195 [00:00<?, ?it/s, v_num=npq2]          \n",
            "Epoch 1:   0%|          | 0/195 [00:00<?, ?it/s, v_num=npq2]\n",
            "Epoch 1:  10%|█         | 20/195 [00:02<00:22,  7.89it/s, v_num=npq2]\n",
            "Epoch 1:  10%|█         | 20/195 [00:02<00:22,  7.86it/s, v_num=npq2]\n",
            "Epoch 1:  21%|██        | 40/195 [00:04<00:18,  8.42it/s, v_num=npq2]\n",
            "Epoch 1:  21%|██        | 40/195 [00:04<00:18,  8.42it/s, v_num=npq2]\n",
            "Epoch 1:  31%|███       | 60/195 [00:06<00:15,  8.79it/s, v_num=npq2]\n",
            "Epoch 1:  31%|███       | 60/195 [00:06<00:15,  8.79it/s, v_num=npq2]\n",
            "Epoch 1:  41%|████      | 80/195 [00:09<00:13,  8.45it/s, v_num=npq2]\n",
            "Epoch 1:  41%|████      | 80/195 [00:09<00:13,  8.44it/s, v_num=npq2]\n",
            "Epoch 1:  51%|█████▏    | 100/195 [00:12<00:12,  7.76it/s, v_num=npq2]\n",
            "Epoch 1:  51%|█████▏    | 100/195 [00:12<00:12,  7.75it/s, v_num=npq2]\n",
            "Epoch 1:  62%|██████▏   | 120/195 [00:16<00:10,  7.22it/s, v_num=npq2]\n",
            "Epoch 1:  62%|██████▏   | 120/195 [00:16<00:10,  7.21it/s, v_num=npq2]\n",
            "Epoch 1:  72%|███████▏  | 140/195 [00:19<00:07,  7.28it/s, v_num=npq2]\n",
            "Epoch 1:  72%|███████▏  | 140/195 [00:19<00:07,  7.28it/s, v_num=npq2]\n",
            "Epoch 1:  82%|████████▏ | 160/195 [00:21<00:04,  7.50it/s, v_num=npq2]\n",
            "Epoch 1:  82%|████████▏ | 160/195 [00:21<00:04,  7.50it/s, v_num=npq2]\n",
            "Epoch 1:  92%|█████████▏| 180/195 [00:23<00:01,  7.74it/s, v_num=npq2]\n",
            "Epoch 1:  92%|█████████▏| 180/195 [00:23<00:01,  7.74it/s, v_num=npq2]\n",
            "Epoch 1: 100%|██████████| 195/195 [00:24<00:00,  7.92it/s, v_num=npq2]\n",
            "Epoch 1: 100%|██████████| 195/195 [00:24<00:00,  7.92it/s, v_num=npq2]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:  50%|█████     | 20/40 [00:01<00:01, 19.06it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 40/40 [00:01<00:00, 20.70it/s]\u001b[A\n",
            "\n",
            "                                                                        \u001b[A\n",
            "Epoch 1: 100%|██████████| 195/195 [00:27<00:00,  7.22it/s, v_num=npq2]\n",
            "Epoch 1: 100%|██████████| 195/195 [00:27<00:00,  7.18it/s, v_num=npq2]\n",
            "Epoch 1:   0%|          | 0/195 [00:00<?, ?it/s, v_num=npq2]          \n",
            "Epoch 2:   0%|          | 0/195 [00:00<?, ?it/s, v_num=npq2]\n",
            "Epoch 2:  10%|█         | 20/195 [00:03<00:32,  5.33it/s, v_num=npq2]\n",
            "Epoch 2:  10%|█         | 20/195 [00:03<00:32,  5.33it/s, v_num=npq2]\n",
            "Epoch 2:  21%|██        | 40/195 [00:07<00:29,  5.25it/s, v_num=npq2]\n",
            "Epoch 2:  21%|██        | 40/195 [00:07<00:29,  5.25it/s, v_num=npq2]\n",
            "Epoch 2:  31%|███       | 60/195 [00:11<00:24,  5.44it/s, v_num=npq2]\n",
            "Epoch 2:  31%|███       | 60/195 [00:11<00:24,  5.44it/s, v_num=npq2]\n",
            "Epoch 2:  41%|████      | 80/195 [00:13<00:18,  6.11it/s, v_num=npq2]\n",
            "Epoch 2:  41%|████      | 80/195 [00:13<00:18,  6.11it/s, v_num=npq2]\n",
            "Epoch 2:  51%|█████▏    | 100/195 [00:15<00:14,  6.65it/s, v_num=npq2]\n",
            "Epoch 2:  51%|█████▏    | 100/195 [00:15<00:14,  6.65it/s, v_num=npq2]\n",
            "Epoch 2:  62%|██████▏   | 120/195 [00:17<00:10,  6.99it/s, v_num=npq2]\n",
            "Epoch 2:  62%|██████▏   | 120/195 [00:17<00:10,  6.99it/s, v_num=npq2]\n",
            "Epoch 2:  72%|███████▏  | 140/195 [00:19<00:07,  7.21it/s, v_num=npq2]\n",
            "Epoch 2:  72%|███████▏  | 140/195 [00:19<00:07,  7.21it/s, v_num=npq2]\n",
            "Epoch 2:  82%|████████▏ | 160/195 [00:21<00:04,  7.34it/s, v_num=npq2]\n",
            "Epoch 2:  82%|████████▏ | 160/195 [00:21<00:04,  7.34it/s, v_num=npq2]\n",
            "Epoch 2:  92%|█████████▏| 180/195 [00:25<00:02,  7.13it/s, v_num=npq2]\n",
            "Epoch 2:  92%|█████████▏| 180/195 [00:25<00:02,  7.13it/s, v_num=npq2]\n",
            "Epoch 2: 100%|██████████| 195/195 [00:28<00:00,  6.96it/s, v_num=npq2]\n",
            "Epoch 2: 100%|██████████| 195/195 [00:28<00:00,  6.96it/s, v_num=npq2]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:  50%|█████     | 20/40 [00:01<00:01, 12.08it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 40/40 [00:02<00:00, 15.20it/s]\u001b[A\n",
            "\n",
            "                                                                        \u001b[A\n",
            "Epoch 2: 100%|██████████| 195/195 [00:31<00:00,  6.20it/s, v_num=npq2]\n",
            "Epoch 2: 100%|██████████| 195/195 [00:31<00:00,  6.17it/s, v_num=npq2]\n",
            "Epoch 2:   0%|          | 0/195 [00:00<?, ?it/s, v_num=npq2]          \n",
            "Epoch 3:   0%|          | 0/195 [00:00<?, ?it/s, v_num=npq2]\n",
            "Epoch 3:  10%|█         | 20/195 [00:02<00:22,  7.79it/s, v_num=npq2]\n",
            "Epoch 3:  10%|█         | 20/195 [00:02<00:22,  7.78it/s, v_num=npq2]\n",
            "Epoch 3:  21%|██        | 40/195 [00:04<00:18,  8.44it/s, v_num=npq2]\n",
            "Epoch 3:  21%|██        | 40/195 [00:04<00:18,  8.44it/s, v_num=npq2]\n",
            "Epoch 3:  31%|███       | 60/195 [00:06<00:15,  8.65it/s, v_num=npq2]\n",
            "Epoch 3:  31%|███       | 60/195 [00:06<00:15,  8.64it/s, v_num=npq2]\n",
            "Epoch 3:  41%|████      | 80/195 [00:08<00:12,  8.94it/s, v_num=npq2]\n",
            "Epoch 3:  41%|████      | 80/195 [00:08<00:12,  8.93it/s, v_num=npq2]\n",
            "Epoch 3:  51%|█████▏    | 100/195 [00:12<00:11,  8.14it/s, v_num=npq2]\n",
            "Epoch 3:  51%|█████▏    | 100/195 [00:12<00:11,  8.13it/s, v_num=npq2]\n",
            "Epoch 3:  62%|██████▏   | 120/195 [00:16<00:10,  7.48it/s, v_num=npq2]\n",
            "Epoch 3:  62%|██████▏   | 120/195 [00:16<00:10,  7.47it/s, v_num=npq2]\n",
            "Epoch 3:  72%|███████▏  | 140/195 [00:19<00:07,  7.22it/s, v_num=npq2]\n",
            "Epoch 3:  72%|███████▏  | 140/195 [00:19<00:07,  7.22it/s, v_num=npq2]\n",
            "Epoch 3:  82%|████████▏ | 160/195 [00:21<00:04,  7.42it/s, v_num=npq2]\n",
            "Epoch 3:  82%|████████▏ | 160/195 [00:21<00:04,  7.42it/s, v_num=npq2]\n",
            "Epoch 3:  92%|█████████▏| 180/195 [00:23<00:01,  7.60it/s, v_num=npq2]\n",
            "Epoch 3:  92%|█████████▏| 180/195 [00:23<00:01,  7.60it/s, v_num=npq2]\n",
            "Epoch 3: 100%|██████████| 195/195 [00:24<00:00,  7.82it/s, v_num=npq2]\n",
            "Epoch 3: 100%|██████████| 195/195 [00:24<00:00,  7.82it/s, v_num=npq2]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:  50%|█████     | 20/40 [00:01<00:01, 19.42it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 40/40 [00:01<00:00, 20.55it/s]\u001b[A\n",
            "\n",
            "                                                                        \u001b[A\n",
            "Epoch 3: 100%|██████████| 195/195 [00:27<00:00,  7.12it/s, v_num=npq2]\n",
            "Epoch 3: 100%|██████████| 195/195 [00:27<00:00,  7.08it/s, v_num=npq2]\n",
            "Epoch 3:   0%|          | 0/195 [00:00<?, ?it/s, v_num=npq2]          \n",
            "Epoch 4:   0%|          | 0/195 [00:00<?, ?it/s, v_num=npq2]\n",
            "Epoch 4:  10%|█         | 20/195 [00:03<00:29,  5.95it/s, v_num=npq2]\n",
            "Epoch 4:  10%|█         | 20/195 [00:03<00:29,  5.94it/s, v_num=npq2]\n",
            "Epoch 4:  21%|██        | 40/195 [00:07<00:27,  5.64it/s, v_num=npq2]\n",
            "Epoch 4:  21%|██        | 40/195 [00:07<00:27,  5.63it/s, v_num=npq2]\n",
            "Epoch 4:  31%|███       | 60/195 [00:10<00:23,  5.65it/s, v_num=npq2]\n",
            "Epoch 4:  31%|███       | 60/195 [00:10<00:23,  5.65it/s, v_num=npq2]\n",
            "Epoch 4:  41%|████      | 80/195 [00:12<00:18,  6.28it/s, v_num=npq2]\n",
            "Epoch 4:  41%|████      | 80/195 [00:12<00:18,  6.27it/s, v_num=npq2]\n",
            "Epoch 4:  51%|█████▏    | 100/195 [00:14<00:14,  6.73it/s, v_num=npq2]\n",
            "Epoch 4:  51%|█████▏    | 100/195 [00:14<00:14,  6.73it/s, v_num=npq2]\n",
            "Epoch 4:  62%|██████▏   | 120/195 [00:16<00:10,  7.15it/s, v_num=npq2]\n",
            "Epoch 4:  62%|██████▏   | 120/195 [00:16<00:10,  7.15it/s, v_num=npq2]\n",
            "Epoch 4:  72%|███████▏  | 140/195 [00:18<00:07,  7.41it/s, v_num=npq2]\n",
            "Epoch 4:  72%|███████▏  | 140/195 [00:18<00:07,  7.41it/s, v_num=npq2]\n",
            "Epoch 4:  82%|████████▏ | 160/195 [00:21<00:04,  7.56it/s, v_num=npq2]\n",
            "Epoch 4:  82%|████████▏ | 160/195 [00:21<00:04,  7.56it/s, v_num=npq2]\n",
            "Epoch 4:  92%|█████████▏| 180/195 [00:24<00:02,  7.30it/s, v_num=npq2]\n",
            "Epoch 4:  92%|█████████▏| 180/195 [00:24<00:02,  7.29it/s, v_num=npq2]\n",
            "Epoch 4: 100%|██████████| 195/195 [00:27<00:00,  7.20it/s, v_num=npq2]\n",
            "Epoch 4: 100%|██████████| 195/195 [00:27<00:00,  7.20it/s, v_num=npq2]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:  50%|█████     | 20/40 [00:01<00:01, 11.17it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 40/40 [00:02<00:00, 13.57it/s]\u001b[A\n",
            "\n",
            "                                                                        \u001b[A\n",
            "Epoch 4: 100%|██████████| 195/195 [00:30<00:00,  6.31it/s, v_num=npq2]\n",
            "Epoch 4: 100%|██████████| 195/195 [00:31<00:00,  6.28it/s, v_num=npq2]\n",
            "Epoch 4: 100%|██████████| 195/195 [00:31<00:00,  6.28it/s, v_num=npq2]\n",
            "/content/solo-learn/solo/backbones/convnext/convnext.py:27: UserWarning: Overwriting convnext_tiny in registry with solo.backbones.convnext.convnext.convnext_tiny. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def convnext_tiny(**kwargs):\n",
            "/content/solo-learn/solo/backbones/convnext/convnext.py:34: UserWarning: Overwriting convnext_small in registry with solo.backbones.convnext.convnext.convnext_small. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def convnext_small(**kwargs):\n",
            "/content/solo-learn/solo/backbones/convnext/convnext.py:41: UserWarning: Overwriting convnext_base in registry with solo.backbones.convnext.convnext.convnext_base. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def convnext_base(**kwargs):\n",
            "/content/solo-learn/solo/backbones/convnext/convnext.py:48: UserWarning: Overwriting convnext_large in registry with solo.backbones.convnext.convnext.convnext_large. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def convnext_large(**kwargs):\n",
            "/content/solo-learn/solo/backbones/poolformer/poolformer.py:422: UserWarning: Overwriting poolformer_s12 in registry with solo.backbones.poolformer.poolformer.poolformer_s12. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def poolformer_s12(**kwargs):\n",
            "/content/solo-learn/solo/backbones/poolformer/poolformer.py:447: UserWarning: Overwriting poolformer_s24 in registry with solo.backbones.poolformer.poolformer.poolformer_s24. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def poolformer_s24(**kwargs):\n",
            "/content/solo-learn/solo/backbones/poolformer/poolformer.py:468: UserWarning: Overwriting poolformer_s36 in registry with solo.backbones.poolformer.poolformer.poolformer_s36. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def poolformer_s36(**kwargs):\n",
            "/content/solo-learn/solo/backbones/poolformer/poolformer.py:490: UserWarning: Overwriting poolformer_m36 in registry with solo.backbones.poolformer.poolformer.poolformer_m36. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def poolformer_m36(**kwargs):\n",
            "/content/solo-learn/solo/backbones/poolformer/poolformer.py:512: UserWarning: Overwriting poolformer_m48 in registry with solo.backbones.poolformer.poolformer.poolformer_m48. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def poolformer_m48(**kwargs):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "wandb: Tracking run with wandb version 0.17.2\n",
            "wandb: W&B syncing is set to `offline` in this directory.  \n",
            "wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.\n",
            "wandb: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
            "/usr/local/lib/python3.10/dist-packages/lightning/fabric/connector.py:565: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "wandb: \n",
            "wandb: Run history:\n",
            "wandb:               epoch ▁▁▁▁▁▃▃▃▃▃▃▅▅▅▅▅▅▆▆▆▆▆▆██████\n",
            "wandb:              lr-SGD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "wandb:    train_acc1_epoch ▁▆▆██\n",
            "wandb:     train_acc1_step █▃▄▃▇▅▅▄▇▃▁▅▃▅▄▅▃▃▄\n",
            "wandb:    train_acc5_epoch ▁▇▅█▆\n",
            "wandb:     train_acc5_step █▃▅▂▅▆▇▇▅▄▃▃█▁▄▃▁▃▃\n",
            "wandb:    train_loss_epoch █▂▂▁▂\n",
            "wandb:     train_loss_step ▂█▄█▂▁▂▃▃▂▃▂▁▅▃▅▆▃▆\n",
            "wandb: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n",
            "wandb:            val_acc1 ▁▆▇▆█\n",
            "wandb:            val_acc5 ▁█▂▁▃\n",
            "wandb:            val_loss █▃▄▄▁\n",
            "wandb: \n",
            "wandb: Run summary:\n",
            "wandb:               epoch 4\n",
            "wandb:              lr-SGD 1.0\n",
            "wandb:    train_acc1_epoch 37.47797\n",
            "wandb:     train_acc1_step 35.15625\n",
            "wandb:    train_acc5_epoch 82.08533\n",
            "wandb:     train_acc5_step 80.46875\n",
            "wandb:    train_loss_epoch 48.9322\n",
            "wandb:     train_loss_step 59.28749\n",
            "wandb: trainer/global_step 974\n",
            "wandb:            val_acc1 48.85\n",
            "wandb:            val_acc5 86.42\n",
            "wandb:            val_loss 30.45418\n",
            "wandb: \n",
            "wandb: You can sync this run to the cloud by running:\n",
            "wandb: wandb sync ./wandb/offline-run-20240622_221423-19ixnpq2\n",
            "wandb: Find logs at: ./wandb/offline-run-20240622_221423-19ixnpq2/logs\n",
            "wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MoCoV3"
      ],
      "metadata": {
        "id": "M49xdF6lfASv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### pretrain the model"
      ],
      "metadata": {
        "id": "1jPiEZt1Ct6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "command = ['python3', 'solo-learn/main_pretrain.py',\n",
        "           '--config-path', 'scripts/pretrain/cifar/',\n",
        "           '--config-name', 'mocov3.yaml',\n",
        "           '++wandb.offline=True']"
      ],
      "metadata": {
        "id": "TM9r252Ku1TE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "\n",
        "for line in process.stdout:\n",
        "    print(line, end='')\n",
        "\n",
        "for line in process.stderr:\n",
        "    print(line, end='')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VpDaKsgvCfW",
        "outputId": "2d829652-d604-4d2a-c68e-8340945c69b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "\n",
            "Training: |          | 0/? [00:00<?, ?it/s]\n",
            "Training:   0%|          | 0/195 [00:00<?, ?it/s]\n",
            "Epoch 0:   0%|          | 0/195 [00:00<?, ?it/s] \n",
            "Epoch 0:  10%|█         | 20/195 [00:14<02:03,  1.41it/s]\n",
            "Epoch 0:  10%|█         | 20/195 [00:14<02:03,  1.41it/s, v_num=8pjp]\n",
            "Epoch 0:  21%|██        | 40/195 [00:24<01:33,  1.66it/s, v_num=8pjp]\n",
            "Epoch 0:  21%|██        | 40/195 [00:24<01:33,  1.66it/s, v_num=8pjp]\n",
            "Epoch 0:  31%|███       | 60/195 [00:36<01:21,  1.66it/s, v_num=8pjp]\n",
            "Epoch 0:  31%|███       | 60/195 [00:36<01:21,  1.66it/s, v_num=8pjp]\n",
            "Epoch 0:  41%|████      | 80/195 [00:48<01:09,  1.66it/s, v_num=8pjp]\n",
            "Epoch 0:  41%|████      | 80/195 [00:48<01:09,  1.66it/s, v_num=8pjp]\n",
            "Epoch 0:  51%|█████▏    | 100/195 [00:58<00:55,  1.71it/s, v_num=8pjp]\n",
            "Epoch 0:  51%|█████▏    | 100/195 [00:58<00:55,  1.71it/s, v_num=8pjp]\n",
            "Epoch 0:  62%|██████▏   | 120/195 [01:09<00:43,  1.72it/s, v_num=8pjp]\n",
            "Epoch 0:  62%|██████▏   | 120/195 [01:09<00:43,  1.72it/s, v_num=8pjp]\n",
            "Epoch 0:  72%|███████▏  | 140/195 [01:22<00:32,  1.70it/s, v_num=8pjp]\n",
            "Epoch 0:  72%|███████▏  | 140/195 [01:22<00:32,  1.70it/s, v_num=8pjp]\n",
            "Epoch 0:  82%|████████▏ | 160/195 [01:31<00:19,  1.75it/s, v_num=8pjp]\n",
            "Epoch 0:  82%|████████▏ | 160/195 [01:31<00:19,  1.75it/s, v_num=8pjp]\n",
            "Epoch 0:  92%|█████████▏| 180/195 [01:43<00:08,  1.74it/s, v_num=8pjp]\n",
            "Epoch 0:  92%|█████████▏| 180/195 [01:43<00:08,  1.74it/s, v_num=8pjp]\n",
            "Epoch 0: 100%|██████████| 195/195 [01:49<00:00,  1.79it/s, v_num=8pjp]\n",
            "Epoch 0: 100%|██████████| 195/195 [01:49<00:00,  1.79it/s, v_num=8pjp]\n",
            "Epoch 0: 100%|██████████| 195/195 [01:49<00:00,  1.77it/s, v_num=8pjp]\n",
            "Epoch 0:   0%|          | 0/195 [00:00<?, ?it/s, v_num=8pjp]          \n",
            "Epoch 1:   0%|          | 0/195 [00:00<?, ?it/s, v_num=8pjp]\n",
            "Epoch 1:  10%|█         | 20/195 [00:13<01:55,  1.52it/s, v_num=8pjp]\n",
            "Epoch 1:  10%|█         | 20/195 [00:13<01:55,  1.52it/s, v_num=8pjp]\n",
            "Epoch 1:  21%|██        | 40/195 [00:25<01:37,  1.58it/s, v_num=8pjp]\n",
            "Epoch 1:  21%|██        | 40/195 [00:25<01:37,  1.58it/s, v_num=8pjp]\n",
            "Epoch 1:  31%|███       | 60/195 [00:35<01:20,  1.68it/s, v_num=8pjp]\n",
            "Epoch 1:  31%|███       | 60/195 [00:35<01:20,  1.68it/s, v_num=8pjp]\n",
            "Epoch 1:  41%|████      | 80/195 [00:46<01:07,  1.70it/s, v_num=8pjp]\n",
            "Epoch 1:  41%|████      | 80/195 [00:46<01:07,  1.70it/s, v_num=8pjp]\n",
            "Epoch 1:  51%|█████▏    | 100/195 [00:59<00:56,  1.69it/s, v_num=8pjp]\n",
            "Epoch 1:  51%|█████▏    | 100/195 [00:59<00:56,  1.69it/s, v_num=8pjp]\n",
            "Epoch 1:  62%|██████▏   | 120/195 [01:10<00:44,  1.70it/s, v_num=8pjp]\n",
            "Epoch 1:  62%|██████▏   | 120/195 [01:10<00:44,  1.70it/s, v_num=8pjp]\n",
            "Epoch 1:  72%|███████▏  | 140/195 [01:21<00:32,  1.72it/s, v_num=8pjp]\n",
            "Epoch 1:  72%|███████▏  | 140/195 [01:21<00:32,  1.72it/s, v_num=8pjp]\n",
            "Epoch 1:  82%|████████▏ | 160/195 [01:33<00:20,  1.71it/s, v_num=8pjp]\n",
            "Epoch 1:  82%|████████▏ | 160/195 [01:33<00:20,  1.71it/s, v_num=8pjp]\n",
            "Epoch 1:  92%|█████████▏| 180/195 [01:45<00:08,  1.71it/s, v_num=8pjp]\n",
            "Epoch 1:  92%|█████████▏| 180/195 [01:45<00:08,  1.71it/s, v_num=8pjp]\n",
            "Epoch 1: 100%|██████████| 195/195 [01:51<00:00,  1.75it/s, v_num=8pjp]\n",
            "Epoch 1: 100%|██████████| 195/195 [01:51<00:00,  1.75it/s, v_num=8pjp]\n",
            "Epoch 1: 100%|██████████| 195/195 [01:52<00:00,  1.74it/s, v_num=8pjp]\n",
            "Epoch 1:   0%|          | 0/195 [00:00<?, ?it/s, v_num=8pjp]          \n",
            "Epoch 2:   0%|          | 0/195 [00:00<?, ?it/s, v_num=8pjp]\n",
            "Epoch 2:  10%|█         | 20/195 [00:14<02:06,  1.38it/s, v_num=8pjp]\n",
            "Epoch 2:  10%|█         | 20/195 [00:14<02:06,  1.38it/s, v_num=8pjp]\n",
            "Epoch 2:  21%|██        | 40/195 [00:26<01:43,  1.49it/s, v_num=8pjp]\n",
            "Epoch 2:  21%|██        | 40/195 [00:26<01:43,  1.49it/s, v_num=8pjp]\n",
            "Epoch 2:  31%|███       | 60/195 [00:36<01:21,  1.65it/s, v_num=8pjp]\n",
            "Epoch 2:  31%|███       | 60/195 [00:36<01:21,  1.65it/s, v_num=8pjp]\n",
            "Epoch 2:  41%|████      | 80/195 [00:49<01:10,  1.62it/s, v_num=8pjp]\n",
            "Epoch 2:  41%|████      | 80/195 [00:49<01:10,  1.62it/s, v_num=8pjp]\n",
            "Epoch 2:  51%|█████▏    | 100/195 [01:01<00:58,  1.63it/s, v_num=8pjp]\n",
            "Epoch 2:  51%|█████▏    | 100/195 [01:01<00:58,  1.63it/s, v_num=8pjp]\n",
            "Epoch 2:  62%|██████▏   | 120/195 [01:11<00:44,  1.68it/s, v_num=8pjp]\n",
            "Epoch 2:  62%|██████▏   | 120/195 [01:11<00:44,  1.68it/s, v_num=8pjp]\n",
            "Epoch 2:  72%|███████▏  | 140/195 [01:23<00:32,  1.68it/s, v_num=8pjp]\n",
            "Epoch 2:  72%|███████▏  | 140/195 [01:23<00:32,  1.68it/s, v_num=8pjp]\n",
            "Epoch 2:  82%|████████▏ | 160/195 [01:35<00:20,  1.67it/s, v_num=8pjp]\n",
            "Epoch 2:  82%|████████▏ | 160/195 [01:35<00:20,  1.67it/s, v_num=8pjp]\n",
            "Epoch 2:  92%|█████████▏| 180/195 [01:46<00:08,  1.69it/s, v_num=8pjp]\n",
            "Epoch 2:  92%|█████████▏| 180/195 [01:46<00:08,  1.69it/s, v_num=8pjp]\n",
            "Epoch 2: 100%|██████████| 195/195 [01:54<00:00,  1.70it/s, v_num=8pjp]\n",
            "Epoch 2: 100%|██████████| 195/195 [01:54<00:00,  1.70it/s, v_num=8pjp]\n",
            "Epoch 2: 100%|██████████| 195/195 [01:55<00:00,  1.69it/s, v_num=8pjp]\n",
            "Epoch 2:   0%|          | 0/195 [00:00<?, ?it/s, v_num=8pjp]          \n",
            "Epoch 3:   0%|          | 0/195 [00:00<?, ?it/s, v_num=8pjp]\n",
            "Epoch 3:  10%|█         | 20/195 [00:13<01:58,  1.47it/s, v_num=8pjp]\n",
            "Epoch 3:  10%|█         | 20/195 [00:13<01:58,  1.47it/s, v_num=8pjp]\n",
            "Epoch 3:  21%|██        | 40/195 [00:23<01:29,  1.73it/s, v_num=8pjp]\n",
            "Epoch 3:  21%|██        | 40/195 [00:23<01:29,  1.73it/s, v_num=8pjp]\n",
            "Epoch 3:  31%|███       | 60/195 [00:36<01:21,  1.66it/s, v_num=8pjp]\n",
            "Epoch 3:  31%|███       | 60/195 [00:36<01:21,  1.66it/s, v_num=8pjp]\n",
            "Epoch 3:  41%|████      | 80/195 [00:48<01:09,  1.65it/s, v_num=8pjp]\n",
            "Epoch 3:  41%|████      | 80/195 [00:48<01:09,  1.65it/s, v_num=8pjp]\n",
            "Epoch 3:  51%|█████▏    | 100/195 [00:59<00:56,  1.68it/s, v_num=8pjp]\n",
            "Epoch 3:  51%|█████▏    | 100/195 [00:59<00:56,  1.68it/s, v_num=8pjp]\n",
            "Epoch 3:  62%|██████▏   | 120/195 [01:10<00:44,  1.70it/s, v_num=8pjp]\n",
            "Epoch 3:  62%|██████▏   | 120/195 [01:10<00:44,  1.70it/s, v_num=8pjp]\n",
            "Epoch 3:  72%|███████▏  | 140/195 [01:22<00:32,  1.69it/s, v_num=8pjp]\n",
            "Epoch 3:  72%|███████▏  | 140/195 [01:22<00:32,  1.69it/s, v_num=8pjp]\n",
            "Epoch 3:  82%|████████▏ | 160/195 [01:34<00:20,  1.70it/s, v_num=8pjp]\n",
            "Epoch 3:  82%|████████▏ | 160/195 [01:34<00:20,  1.70it/s, v_num=8pjp]\n",
            "Epoch 3:  92%|█████████▏| 180/195 [01:44<00:08,  1.73it/s, v_num=8pjp]\n",
            "Epoch 3:  92%|█████████▏| 180/195 [01:44<00:08,  1.73it/s, v_num=8pjp]\n",
            "Epoch 3: 100%|██████████| 195/195 [01:53<00:00,  1.72it/s, v_num=8pjp]\n",
            "Epoch 3: 100%|██████████| 195/195 [01:53<00:00,  1.72it/s, v_num=8pjp]\n",
            "Epoch 3: 100%|██████████| 195/195 [01:53<00:00,  1.71it/s, v_num=8pjp]\n",
            "Epoch 3:   0%|          | 0/195 [00:00<?, ?it/s, v_num=8pjp]          \n",
            "Epoch 4:   0%|          | 0/195 [00:00<?, ?it/s, v_num=8pjp]\n",
            "Epoch 4:  10%|█         | 20/195 [00:13<01:57,  1.50it/s, v_num=8pjp]\n",
            "Epoch 4:  10%|█         | 20/195 [00:13<01:57,  1.49it/s, v_num=8pjp]\n",
            "Epoch 4:  21%|██        | 40/195 [00:23<01:30,  1.71it/s, v_num=8pjp]\n",
            "Epoch 4:  21%|██        | 40/195 [00:23<01:30,  1.71it/s, v_num=8pjp]\n",
            "Epoch 4:  31%|███       | 60/195 [00:36<01:21,  1.66it/s, v_num=8pjp]\n",
            "Epoch 4:  31%|███       | 60/195 [00:36<01:21,  1.66it/s, v_num=8pjp]\n",
            "Epoch 4:  41%|████      | 80/195 [00:48<01:09,  1.66it/s, v_num=8pjp]\n",
            "Epoch 4:  41%|████      | 80/195 [00:48<01:09,  1.66it/s, v_num=8pjp]\n",
            "Epoch 4:  51%|█████▏    | 100/195 [00:59<00:56,  1.68it/s, v_num=8pjp]\n",
            "Epoch 4:  51%|█████▏    | 100/195 [00:59<00:56,  1.68it/s, v_num=8pjp]\n",
            "Epoch 4:  62%|██████▏   | 120/195 [01:11<00:44,  1.67it/s, v_num=8pjp]\n",
            "Epoch 4:  62%|██████▏   | 120/195 [01:11<00:44,  1.67it/s, v_num=8pjp]\n",
            "Epoch 4:  72%|███████▏  | 140/195 [01:24<00:33,  1.66it/s, v_num=8pjp]\n",
            "Epoch 4:  72%|███████▏  | 140/195 [01:24<00:33,  1.66it/s, v_num=8pjp]\n",
            "Epoch 4:  82%|████████▏ | 160/195 [01:34<00:20,  1.69it/s, v_num=8pjp]\n",
            "Epoch 4:  82%|████████▏ | 160/195 [01:34<00:20,  1.69it/s, v_num=8pjp]\n",
            "Epoch 4:  92%|█████████▏| 180/195 [01:46<00:08,  1.70it/s, v_num=8pjp]\n",
            "Epoch 4:  92%|█████████▏| 180/195 [01:46<00:08,  1.70it/s, v_num=8pjp]\n",
            "Epoch 4: 100%|██████████| 195/195 [01:53<00:00,  1.72it/s, v_num=8pjp]\n",
            "Epoch 4: 100%|██████████| 195/195 [01:53<00:00,  1.72it/s, v_num=8pjp]\n",
            "Epoch 4: 100%|██████████| 195/195 [01:54<00:00,  1.70it/s, v_num=8pjp]\n",
            "Epoch 4: 100%|██████████| 195/195 [01:54<00:00,  1.70it/s, v_num=8pjp]\n",
            "/content/solo-learn/solo/backbones/convnext/convnext.py:27: UserWarning: Overwriting convnext_tiny in registry with solo.backbones.convnext.convnext.convnext_tiny. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def convnext_tiny(**kwargs):\n",
            "/content/solo-learn/solo/backbones/convnext/convnext.py:34: UserWarning: Overwriting convnext_small in registry with solo.backbones.convnext.convnext.convnext_small. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def convnext_small(**kwargs):\n",
            "/content/solo-learn/solo/backbones/convnext/convnext.py:41: UserWarning: Overwriting convnext_base in registry with solo.backbones.convnext.convnext.convnext_base. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def convnext_base(**kwargs):\n",
            "/content/solo-learn/solo/backbones/convnext/convnext.py:48: UserWarning: Overwriting convnext_large in registry with solo.backbones.convnext.convnext.convnext_large. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def convnext_large(**kwargs):\n",
            "/content/solo-learn/solo/backbones/poolformer/poolformer.py:422: UserWarning: Overwriting poolformer_s12 in registry with solo.backbones.poolformer.poolformer.poolformer_s12. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def poolformer_s12(**kwargs):\n",
            "/content/solo-learn/solo/backbones/poolformer/poolformer.py:447: UserWarning: Overwriting poolformer_s24 in registry with solo.backbones.poolformer.poolformer.poolformer_s24. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def poolformer_s24(**kwargs):\n",
            "/content/solo-learn/solo/backbones/poolformer/poolformer.py:468: UserWarning: Overwriting poolformer_s36 in registry with solo.backbones.poolformer.poolformer.poolformer_s36. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def poolformer_s36(**kwargs):\n",
            "/content/solo-learn/solo/backbones/poolformer/poolformer.py:490: UserWarning: Overwriting poolformer_m36 in registry with solo.backbones.poolformer.poolformer.poolformer_m36. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def poolformer_m36(**kwargs):\n",
            "/content/solo-learn/solo/backbones/poolformer/poolformer.py:512: UserWarning: Overwriting poolformer_m48 in registry with solo.backbones.poolformer.poolformer.poolformer_m48. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def poolformer_m48(**kwargs):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "wandb: Tracking run with wandb version 0.17.2\n",
            "wandb: W&B syncing is set to `offline` in this directory.  \n",
            "wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.\n",
            "wandb: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
            "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "wandb: \n",
            "wandb: Run history:\n",
            "wandb:                        epoch ▁▁▁▁▃▃▃▃▃▅▅▅▅▅▆▆▆▆▆█████\n",
            "wandb:             lr-LARS/backbone ▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██\n",
            "wandb:           lr-LARS/classifier ▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██\n",
            "wandb:            lr-LARS/predictor ▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██\n",
            "wandb:            lr-LARS/projector ▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██\n",
            "wandb:                          tau ▁▁▁▂▂▂▃▃▄▅▅▆▆▇▇▇███\n",
            "wandb:             train_acc1_epoch ▁▅▆▇█\n",
            "wandb:              train_acc1_step ▁▃▄▃▃▆▆▆▆▅▆▆█▆█▆▇▆▇\n",
            "wandb:             train_acc5_epoch ▁▆▇██\n",
            "wandb:              train_acc5_step ▁▄▅▅▆▇▇▆▇▆▇▇▇▇███▇█\n",
            "wandb:       train_class_loss_epoch █▄▃▂▁\n",
            "wandb:        train_class_loss_step █▆▅▅▅▃▃▅▂▄▂▃▁▃▂▃▂▃▃\n",
            "wandb: train_contrastive_loss_epoch █▄▂▁▁\n",
            "wandb:  train_contrastive_loss_step █▆▆▅▄▃▃▂▃▂▂▂▁▁▁▂▂▁▁\n",
            "wandb:          trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██\n",
            "wandb: \n",
            "wandb: Run summary:\n",
            "wandb:                        epoch 4\n",
            "wandb:             lr-LARS/backbone 0.14609\n",
            "wandb:           lr-LARS/classifier 0.14609\n",
            "wandb:            lr-LARS/predictor 0.14609\n",
            "wandb:            lr-LARS/projector 0.14609\n",
            "wandb:                          tau 0.99998\n",
            "wandb:             train_acc1_epoch 35.41767\n",
            "wandb:              train_acc1_step 33.39844\n",
            "wandb:             train_acc5_epoch 85.23037\n",
            "wandb:              train_acc5_step 85.54688\n",
            "wandb:       train_class_loss_epoch 1.80868\n",
            "wandb:        train_class_loss_step 1.88569\n",
            "wandb: train_contrastive_loss_epoch 3.0116\n",
            "wandb:  train_contrastive_loss_step 2.98492\n",
            "wandb:          trainer/global_step 974\n",
            "wandb: \n",
            "wandb: You can sync this run to the cloud by running:\n",
            "wandb: wandb sync ./wandb/offline-run-20240622_221743-jgby8pjp\n",
            "wandb: Find logs at: ./wandb/offline-run-20240622_221743-jgby8pjp/logs\n",
            "wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### evaluate the model"
      ],
      "metadata": {
        "id": "tBMql_8FCwYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for dirpath, _, filenames in os.walk(os.path.join(os.getcwd(), 'trained_models', 'mocov3')):\n",
        "  if filenames:\n",
        "    assert filenames[1].endswith('ckpt')\n",
        "    new_path = os.path.join(dirpath, filenames[1].replace('=', '_'))\n",
        "    os.rename(os.path.join(dirpath, filenames[1]), new_path)"
      ],
      "metadata": {
        "id": "eu9KQPaJv-Tl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "command_path_name = '/'.join(new_path.split('/')[2:])"
      ],
      "metadata": {
        "id": "JM_wbgETv-Qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "command = ['python3', 'solo-learn/main_linear.py',\n",
        "           '--config-path', 'scripts/linear/cifar/',\n",
        "           '--config-name', 'mocov3.yaml',\n",
        "           f'++pretrained_feature_extractor={command_path_name}',\n",
        "           '++wandb.offline=True']"
      ],
      "metadata": {
        "id": "NeqU5IuZv-Nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "\n",
        "for line in process.stdout:\n",
        "    print(line, end='')\n",
        "\n",
        "for line in process.stderr:\n",
        "    print(line, end='')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoiheLJLv-Fc",
        "outputId": "46dc7d36-3de2-46ae-8ef4-9556e86cf05b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Sanity Checking: |          | 0/? [00:00<?, ?it/s]\n",
            "Sanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  3.43it/s]\n",
            "                                                                           \n",
            "\n",
            "Training: |          | 0/? [00:00<?, ?it/s]\n",
            "Training:   0%|          | 0/195 [00:00<?, ?it/s]\n",
            "Epoch 0:   0%|          | 0/195 [00:00<?, ?it/s] \n",
            "Epoch 0:  10%|█         | 20/195 [00:02<00:21,  8.10it/s]\n",
            "Epoch 0:  10%|█         | 20/195 [00:02<00:21,  8.06it/s, v_num=nhtr]\n",
            "Epoch 0:  21%|██        | 40/195 [00:04<00:17,  8.94it/s, v_num=nhtr]\n",
            "Epoch 0:  21%|██        | 40/195 [00:04<00:17,  8.93it/s, v_num=nhtr]\n",
            "Epoch 0:  31%|███       | 60/195 [00:07<00:16,  8.18it/s, v_num=nhtr]\n",
            "Epoch 0:  31%|███       | 60/195 [00:07<00:16,  8.17it/s, v_num=nhtr]\n",
            "Epoch 0:  41%|████      | 80/195 [00:10<00:15,  7.39it/s, v_num=nhtr]\n",
            "Epoch 0:  41%|████      | 80/195 [00:10<00:15,  7.38it/s, v_num=nhtr]\n",
            "Epoch 0:  51%|█████▏    | 100/195 [00:15<00:14,  6.65it/s, v_num=nhtr]\n",
            "Epoch 0:  51%|█████▏    | 100/195 [00:15<00:14,  6.65it/s, v_num=nhtr]\n",
            "Epoch 0:  62%|██████▏   | 120/195 [00:16<00:10,  7.08it/s, v_num=nhtr]\n",
            "Epoch 0:  62%|██████▏   | 120/195 [00:16<00:10,  7.08it/s, v_num=nhtr]\n",
            "Epoch 0:  72%|███████▏  | 140/195 [00:19<00:07,  7.33it/s, v_num=nhtr]\n",
            "Epoch 0:  72%|███████▏  | 140/195 [00:19<00:07,  7.33it/s, v_num=nhtr]\n",
            "Epoch 0:  82%|████████▏ | 160/195 [00:21<00:04,  7.51it/s, v_num=nhtr]\n",
            "Epoch 0:  82%|████████▏ | 160/195 [00:21<00:04,  7.51it/s, v_num=nhtr]\n",
            "Epoch 0:  92%|█████████▏| 180/195 [00:23<00:01,  7.67it/s, v_num=nhtr]\n",
            "Epoch 0:  92%|█████████▏| 180/195 [00:23<00:01,  7.67it/s, v_num=nhtr]\n",
            "Epoch 0: 100%|██████████| 195/195 [00:24<00:00,  7.89it/s, v_num=nhtr]\n",
            "Epoch 0: 100%|██████████| 195/195 [00:24<00:00,  7.89it/s, v_num=nhtr]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:  50%|█████     | 20/40 [00:01<00:01, 12.52it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 40/40 [00:03<00:00, 11.98it/s]\u001b[A\n",
            "\n",
            "                                                                        \u001b[A\n",
            "Epoch 0: 100%|██████████| 195/195 [00:28<00:00,  6.77it/s, v_num=nhtr]\n",
            "Epoch 0: 100%|██████████| 195/195 [00:29<00:00,  6.72it/s, v_num=nhtr]\n",
            "Epoch 0:   0%|          | 0/195 [00:00<?, ?it/s, v_num=nhtr]          \n",
            "Epoch 1:   0%|          | 0/195 [00:00<?, ?it/s, v_num=nhtr]\n",
            "Epoch 1:  10%|█         | 20/195 [00:04<00:37,  4.72it/s, v_num=nhtr]\n",
            "Epoch 1:  10%|█         | 20/195 [00:04<00:37,  4.72it/s, v_num=nhtr]\n",
            "Epoch 1:  21%|██        | 40/195 [00:06<00:24,  6.35it/s, v_num=nhtr]\n",
            "Epoch 1:  21%|██        | 40/195 [00:06<00:24,  6.35it/s, v_num=nhtr]\n",
            "Epoch 1:  31%|███       | 60/195 [00:08<00:19,  7.02it/s, v_num=nhtr]\n",
            "Epoch 1:  31%|███       | 60/195 [00:08<00:19,  7.02it/s, v_num=nhtr]\n",
            "Epoch 1:  41%|████      | 80/195 [00:10<00:15,  7.56it/s, v_num=nhtr]\n",
            "Epoch 1:  41%|████      | 80/195 [00:10<00:15,  7.55it/s, v_num=nhtr]\n",
            "Epoch 1:  51%|█████▏    | 100/195 [00:12<00:12,  7.90it/s, v_num=nhtr]\n",
            "Epoch 1:  51%|█████▏    | 100/195 [00:12<00:12,  7.90it/s, v_num=nhtr]\n",
            "Epoch 1:  62%|██████▏   | 120/195 [00:15<00:09,  7.94it/s, v_num=nhtr]\n",
            "Epoch 1:  62%|██████▏   | 120/195 [00:15<00:09,  7.94it/s, v_num=nhtr]\n",
            "Epoch 1:  72%|███████▏  | 140/195 [00:18<00:07,  7.48it/s, v_num=nhtr]\n",
            "Epoch 1:  72%|███████▏  | 140/195 [00:18<00:07,  7.48it/s, v_num=nhtr]\n",
            "Epoch 1:  82%|████████▏ | 160/195 [00:22<00:04,  7.12it/s, v_num=nhtr]\n",
            "Epoch 1:  82%|████████▏ | 160/195 [00:22<00:04,  7.12it/s, v_num=nhtr]\n",
            "Epoch 1:  92%|█████████▏| 180/195 [00:25<00:02,  7.19it/s, v_num=nhtr]\n",
            "Epoch 1:  92%|█████████▏| 180/195 [00:25<00:02,  7.19it/s, v_num=nhtr]\n",
            "Epoch 1: 100%|██████████| 195/195 [00:26<00:00,  7.40it/s, v_num=nhtr]\n",
            "Epoch 1: 100%|██████████| 195/195 [00:26<00:00,  7.40it/s, v_num=nhtr]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:  50%|█████     | 20/40 [00:01<00:01, 16.96it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 40/40 [00:02<00:00, 18.87it/s]\u001b[A\n",
            "\n",
            "                                                                        \u001b[A\n",
            "Epoch 1: 100%|██████████| 195/195 [00:29<00:00,  6.71it/s, v_num=nhtr]\n",
            "Epoch 1: 100%|██████████| 195/195 [00:29<00:00,  6.68it/s, v_num=nhtr]\n",
            "Epoch 1:   0%|          | 0/195 [00:00<?, ?it/s, v_num=nhtr]          \n",
            "Epoch 2:   0%|          | 0/195 [00:00<?, ?it/s, v_num=nhtr]\n",
            "Epoch 2:  10%|█         | 20/195 [00:02<00:22,  7.68it/s, v_num=nhtr]\n",
            "Epoch 2:  10%|█         | 20/195 [00:02<00:22,  7.64it/s, v_num=nhtr]\n",
            "Epoch 2:  21%|██        | 40/195 [00:04<00:19,  8.00it/s, v_num=nhtr]\n",
            "Epoch 2:  21%|██        | 40/195 [00:05<00:19,  8.00it/s, v_num=nhtr]\n",
            "Epoch 2:  31%|███       | 60/195 [00:08<00:19,  7.03it/s, v_num=nhtr]\n",
            "Epoch 2:  31%|███       | 60/195 [00:08<00:19,  7.03it/s, v_num=nhtr]\n",
            "Epoch 2:  41%|████      | 80/195 [00:12<00:17,  6.56it/s, v_num=nhtr]\n",
            "Epoch 2:  41%|████      | 80/195 [00:12<00:17,  6.55it/s, v_num=nhtr]\n",
            "Epoch 2:  51%|█████▏    | 100/195 [00:14<00:13,  7.00it/s, v_num=nhtr]\n",
            "Epoch 2:  51%|█████▏    | 100/195 [00:14<00:13,  6.99it/s, v_num=nhtr]\n",
            "Epoch 2:  62%|██████▏   | 120/195 [00:16<00:10,  7.38it/s, v_num=nhtr]\n",
            "Epoch 2:  62%|██████▏   | 120/195 [00:16<00:10,  7.38it/s, v_num=nhtr]\n",
            "Epoch 2:  72%|███████▏  | 140/195 [00:18<00:07,  7.58it/s, v_num=nhtr]\n",
            "Epoch 2:  72%|███████▏  | 140/195 [00:18<00:07,  7.58it/s, v_num=nhtr]\n",
            "Epoch 2:  82%|████████▏ | 160/195 [00:20<00:04,  7.75it/s, v_num=nhtr]\n",
            "Epoch 2:  82%|████████▏ | 160/195 [00:20<00:04,  7.75it/s, v_num=nhtr]\n",
            "Epoch 2:  92%|█████████▏| 180/195 [00:23<00:01,  7.79it/s, v_num=nhtr]\n",
            "Epoch 2:  92%|█████████▏| 180/195 [00:23<00:01,  7.79it/s, v_num=nhtr]\n",
            "Epoch 2: 100%|██████████| 195/195 [00:25<00:00,  7.70it/s, v_num=nhtr]\n",
            "Epoch 2: 100%|██████████| 195/195 [00:25<00:00,  7.70it/s, v_num=nhtr]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:  50%|█████     | 20/40 [00:01<00:01, 11.56it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 40/40 [00:03<00:00, 11.60it/s]\u001b[A\n",
            "\n",
            "                                                                        \u001b[A\n",
            "Epoch 2: 100%|██████████| 195/195 [00:29<00:00,  6.60it/s, v_num=nhtr]\n",
            "Epoch 2: 100%|██████████| 195/195 [00:29<00:00,  6.56it/s, v_num=nhtr]\n",
            "Epoch 2:   0%|          | 0/195 [00:00<?, ?it/s, v_num=nhtr]          \n",
            "Epoch 3:   0%|          | 0/195 [00:00<?, ?it/s, v_num=nhtr]\n",
            "Epoch 3:  10%|█         | 20/195 [00:02<00:21,  7.98it/s, v_num=nhtr]\n",
            "Epoch 3:  10%|█         | 20/195 [00:02<00:21,  7.97it/s, v_num=nhtr]\n",
            "Epoch 3:  21%|██        | 40/195 [00:04<00:18,  8.34it/s, v_num=nhtr]\n",
            "Epoch 3:  21%|██        | 40/195 [00:04<00:18,  8.32it/s, v_num=nhtr]\n",
            "Epoch 3:  31%|███       | 60/195 [00:06<00:15,  8.59it/s, v_num=nhtr]\n",
            "Epoch 3:  31%|███       | 60/195 [00:06<00:15,  8.58it/s, v_num=nhtr]\n",
            "Epoch 3:  41%|████      | 80/195 [00:09<00:13,  8.78it/s, v_num=nhtr]\n",
            "Epoch 3:  41%|████      | 80/195 [00:09<00:13,  8.78it/s, v_num=nhtr]\n",
            "Epoch 3:  51%|█████▏    | 100/195 [00:11<00:11,  8.34it/s, v_num=nhtr]\n",
            "Epoch 3:  51%|█████▏    | 100/195 [00:11<00:11,  8.34it/s, v_num=nhtr]\n",
            "Epoch 3:  62%|██████▏   | 120/195 [00:15<00:09,  7.64it/s, v_num=nhtr]\n",
            "Epoch 3:  62%|██████▏   | 120/195 [00:15<00:09,  7.63it/s, v_num=nhtr]\n",
            "Epoch 3:  72%|███████▏  | 140/195 [00:19<00:07,  7.12it/s, v_num=nhtr]\n",
            "Epoch 3:  72%|███████▏  | 140/195 [00:19<00:07,  7.11it/s, v_num=nhtr]\n",
            "Epoch 3:  82%|████████▏ | 160/195 [00:21<00:04,  7.40it/s, v_num=nhtr]\n",
            "Epoch 3:  82%|████████▏ | 160/195 [00:21<00:04,  7.40it/s, v_num=nhtr]\n",
            "Epoch 3:  92%|█████████▏| 180/195 [00:23<00:01,  7.55it/s, v_num=nhtr]\n",
            "Epoch 3:  92%|█████████▏| 180/195 [00:23<00:01,  7.55it/s, v_num=nhtr]\n",
            "Epoch 3: 100%|██████████| 195/195 [00:25<00:00,  7.72it/s, v_num=nhtr]\n",
            "Epoch 3: 100%|██████████| 195/195 [00:25<00:00,  7.72it/s, v_num=nhtr]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:  50%|█████     | 20/40 [00:01<00:01, 19.87it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 40/40 [00:01<00:00, 20.15it/s]\u001b[A\n",
            "\n",
            "                                                                        \u001b[A\n",
            "Epoch 3: 100%|██████████| 195/195 [00:27<00:00,  7.00it/s, v_num=nhtr]\n",
            "Epoch 3: 100%|██████████| 195/195 [00:28<00:00,  6.96it/s, v_num=nhtr]\n",
            "Epoch 3:   0%|          | 0/195 [00:00<?, ?it/s, v_num=nhtr]          \n",
            "Epoch 4:   0%|          | 0/195 [00:00<?, ?it/s, v_num=nhtr]\n",
            "Epoch 4:  10%|█         | 20/195 [00:03<00:28,  6.23it/s, v_num=nhtr]\n",
            "Epoch 4:  10%|█         | 20/195 [00:03<00:28,  6.23it/s, v_num=nhtr]\n",
            "Epoch 4:  21%|██        | 40/195 [00:06<00:26,  5.90it/s, v_num=nhtr]\n",
            "Epoch 4:  21%|██        | 40/195 [00:06<00:26,  5.89it/s, v_num=nhtr]\n",
            "Epoch 4:  31%|███       | 60/195 [00:10<00:24,  5.60it/s, v_num=nhtr]\n",
            "Epoch 4:  31%|███       | 60/195 [00:10<00:24,  5.59it/s, v_num=nhtr]\n",
            "Epoch 4:  41%|████      | 80/195 [00:13<00:19,  6.04it/s, v_num=nhtr]\n",
            "Epoch 4:  41%|████      | 80/195 [00:13<00:19,  6.04it/s, v_num=nhtr]\n",
            "Epoch 4:  51%|█████▏    | 100/195 [00:15<00:14,  6.51it/s, v_num=nhtr]\n",
            "Epoch 4:  51%|█████▏    | 100/195 [00:15<00:14,  6.51it/s, v_num=nhtr]\n",
            "Epoch 4:  62%|██████▏   | 120/195 [00:17<00:10,  6.86it/s, v_num=nhtr]\n",
            "Epoch 4:  62%|██████▏   | 120/195 [00:17<00:10,  6.86it/s, v_num=nhtr]\n",
            "Epoch 4:  72%|███████▏  | 140/195 [00:19<00:07,  7.14it/s, v_num=nhtr]\n",
            "Epoch 4:  72%|███████▏  | 140/195 [00:19<00:07,  7.14it/s, v_num=nhtr]\n",
            "Epoch 4:  82%|████████▏ | 160/195 [00:21<00:04,  7.38it/s, v_num=nhtr]\n",
            "Epoch 4:  82%|████████▏ | 160/195 [00:21<00:04,  7.37it/s, v_num=nhtr]\n",
            "Epoch 4:  92%|█████████▏| 180/195 [00:24<00:02,  7.21it/s, v_num=nhtr]\n",
            "Epoch 4:  92%|█████████▏| 180/195 [00:24<00:02,  7.21it/s, v_num=nhtr]\n",
            "Epoch 4: 100%|██████████| 195/195 [00:27<00:00,  7.13it/s, v_num=nhtr]\n",
            "Epoch 4: 100%|██████████| 195/195 [00:27<00:00,  7.13it/s, v_num=nhtr]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:  50%|█████     | 20/40 [00:01<00:01, 10.98it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 40/40 [00:02<00:00, 13.44it/s]\u001b[A\n",
            "\n",
            "                                                                        \u001b[A\n",
            "Epoch 4: 100%|██████████| 195/195 [00:31<00:00,  6.24it/s, v_num=nhtr]\n",
            "Epoch 4: 100%|██████████| 195/195 [00:31<00:00,  6.21it/s, v_num=nhtr]\n",
            "Epoch 4: 100%|██████████| 195/195 [00:31<00:00,  6.21it/s, v_num=nhtr]\n",
            "/content/solo-learn/solo/backbones/convnext/convnext.py:27: UserWarning: Overwriting convnext_tiny in registry with solo.backbones.convnext.convnext.convnext_tiny. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def convnext_tiny(**kwargs):\n",
            "/content/solo-learn/solo/backbones/convnext/convnext.py:34: UserWarning: Overwriting convnext_small in registry with solo.backbones.convnext.convnext.convnext_small. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def convnext_small(**kwargs):\n",
            "/content/solo-learn/solo/backbones/convnext/convnext.py:41: UserWarning: Overwriting convnext_base in registry with solo.backbones.convnext.convnext.convnext_base. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def convnext_base(**kwargs):\n",
            "/content/solo-learn/solo/backbones/convnext/convnext.py:48: UserWarning: Overwriting convnext_large in registry with solo.backbones.convnext.convnext.convnext_large. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def convnext_large(**kwargs):\n",
            "/content/solo-learn/solo/backbones/poolformer/poolformer.py:422: UserWarning: Overwriting poolformer_s12 in registry with solo.backbones.poolformer.poolformer.poolformer_s12. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def poolformer_s12(**kwargs):\n",
            "/content/solo-learn/solo/backbones/poolformer/poolformer.py:447: UserWarning: Overwriting poolformer_s24 in registry with solo.backbones.poolformer.poolformer.poolformer_s24. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def poolformer_s24(**kwargs):\n",
            "/content/solo-learn/solo/backbones/poolformer/poolformer.py:468: UserWarning: Overwriting poolformer_s36 in registry with solo.backbones.poolformer.poolformer.poolformer_s36. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def poolformer_s36(**kwargs):\n",
            "/content/solo-learn/solo/backbones/poolformer/poolformer.py:490: UserWarning: Overwriting poolformer_m36 in registry with solo.backbones.poolformer.poolformer.poolformer_m36. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def poolformer_m36(**kwargs):\n",
            "/content/solo-learn/solo/backbones/poolformer/poolformer.py:512: UserWarning: Overwriting poolformer_m48 in registry with solo.backbones.poolformer.poolformer.poolformer_m48. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def poolformer_m48(**kwargs):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "wandb: Tracking run with wandb version 0.17.2\n",
            "wandb: W&B syncing is set to `offline` in this directory.  \n",
            "wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.\n",
            "wandb: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
            "/usr/local/lib/python3.10/dist-packages/lightning/fabric/connector.py:565: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "wandb: \n",
            "wandb: Run history:\n",
            "wandb:               epoch ▁▁▁▁▁▃▃▃▃▃▃▅▅▅▅▅▅▆▆▆▆▆▆██████\n",
            "wandb:              lr-SGD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "wandb:    train_acc1_epoch ▁▄▅██\n",
            "wandb:     train_acc1_step ▇▄▁▇▁▆▅█▇▅▄▂▂▃▆▅█▆▄\n",
            "wandb:    train_acc5_epoch ▁▃▆▇█\n",
            "wandb:     train_acc5_step █▄▃▁▄▆▂▅▅▃▂▄▇▆▃▅▇▄▃\n",
            "wandb:    train_loss_epoch █▅▃▁▂\n",
            "wandb:     train_loss_step ▂▄▅▅▇▂█▂▂▃▄▄▃▂▂▃▁▂▃\n",
            "wandb: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n",
            "wandb:            val_acc1 ▁█▇▄█\n",
            "wandb:            val_acc5 ▂▄█▁▃\n",
            "wandb:            val_loss ▇▁▂█▁\n",
            "wandb: \n",
            "wandb: Run summary:\n",
            "wandb:               epoch 4\n",
            "wandb:              lr-SGD 0.3\n",
            "wandb:    train_acc1_epoch 33.72596\n",
            "wandb:     train_acc1_step 30.46875\n",
            "wandb:    train_acc5_epoch 81.21594\n",
            "wandb:     train_acc5_step 77.34375\n",
            "wandb:    train_loss_epoch 20.03542\n",
            "wandb:     train_loss_step 18.75303\n",
            "wandb: trainer/global_step 974\n",
            "wandb:            val_acc1 40.7\n",
            "wandb:            val_acc5 81.32999\n",
            "wandb:            val_loss 14.02367\n",
            "wandb: \n",
            "wandb: You can sync this run to the cloud by running:\n",
            "wandb: wandb sync ./wandb/offline-run-20240622_222857-ueeynhtr\n",
            "wandb: Find logs at: ./wandb/offline-run-20240622_222857-ueeynhtr/logs\n",
            "wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DINO"
      ],
      "metadata": {
        "id": "tyCahwf8k5-3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### pretrain the model"
      ],
      "metadata": {
        "id": "hEJdoNl8CyUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "command = ['python3', 'solo-learn/main_pretrain.py',\n",
        "           '--config-path', 'scripts/pretrain/cifar/',\n",
        "           '--config-name', 'dino.yaml',\n",
        "           '++wandb.offline=True']"
      ],
      "metadata": {
        "id": "6qcUCe4qvk9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "\n",
        "for line in process.stdout:\n",
        "    print(line, end='')\n",
        "\n",
        "for line in process.stderr:\n",
        "    print(line, end='')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDVNgXoBvp7i",
        "outputId": "b85187c4-9e2b-4331-8579-a6104038f47e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "\n",
            "Training: |          | 0/? [00:00<?, ?it/s]\n",
            "Training:   0%|          | 0/195 [00:00<?, ?it/s]\n",
            "Epoch 0:   0%|          | 0/195 [00:00<?, ?it/s] \n",
            "Epoch 0:  10%|█         | 20/195 [00:14<02:08,  1.36it/s]\n",
            "Epoch 0:  10%|█         | 20/195 [00:14<02:08,  1.36it/s, v_num=56da]\n",
            "Epoch 0:  21%|██        | 40/195 [00:24<01:36,  1.61it/s, v_num=56da]\n",
            "Epoch 0:  21%|██        | 40/195 [00:24<01:36,  1.61it/s, v_num=56da]\n",
            "Epoch 0:  31%|███       | 60/195 [00:35<01:20,  1.67it/s, v_num=56da]\n",
            "Epoch 0:  31%|███       | 60/195 [00:35<01:20,  1.67it/s, v_num=56da]\n",
            "Epoch 0:  41%|████      | 80/195 [00:48<01:09,  1.65it/s, v_num=56da]\n",
            "Epoch 0:  41%|████      | 80/195 [00:48<01:09,  1.65it/s, v_num=56da]\n",
            "Epoch 0:  51%|█████▏    | 100/195 [00:59<00:56,  1.67it/s, v_num=56da]\n",
            "Epoch 0:  51%|█████▏    | 100/195 [00:59<00:56,  1.67it/s, v_num=56da]\n",
            "Epoch 0:  62%|██████▏   | 120/195 [01:10<00:43,  1.71it/s, v_num=56da]\n",
            "Epoch 0:  62%|██████▏   | 120/195 [01:10<00:43,  1.71it/s, v_num=56da]\n",
            "Epoch 0:  72%|███████▏  | 140/195 [01:22<00:32,  1.70it/s, v_num=56da]\n",
            "Epoch 0:  72%|███████▏  | 140/195 [01:22<00:32,  1.69it/s, v_num=56da]\n",
            "Epoch 0:  82%|████████▏ | 160/195 [01:35<00:20,  1.68it/s, v_num=56da]\n",
            "Epoch 0:  82%|████████▏ | 160/195 [01:35<00:20,  1.68it/s, v_num=56da]\n",
            "Epoch 0:  92%|█████████▏| 180/195 [01:44<00:08,  1.72it/s, v_num=56da]\n",
            "Epoch 0:  92%|█████████▏| 180/195 [01:44<00:08,  1.72it/s, v_num=56da]\n",
            "Epoch 0: 100%|██████████| 195/195 [01:53<00:00,  1.72it/s, v_num=56da]\n",
            "Epoch 0: 100%|██████████| 195/195 [01:53<00:00,  1.72it/s, v_num=56da]\n",
            "Epoch 0: 100%|██████████| 195/195 [01:53<00:00,  1.72it/s, v_num=56da]\n",
            "Epoch 0:   0%|          | 0/195 [00:00<?, ?it/s, v_num=56da]          \n",
            "Epoch 1:   0%|          | 0/195 [00:00<?, ?it/s, v_num=56da]\n",
            "Epoch 1:  10%|█         | 20/195 [00:12<01:53,  1.54it/s, v_num=56da]\n",
            "Epoch 1:  10%|█         | 20/195 [00:12<01:53,  1.54it/s, v_num=56da]\n",
            "Epoch 1:  21%|██        | 40/195 [00:23<01:30,  1.71it/s, v_num=56da]\n",
            "Epoch 1:  21%|██        | 40/195 [00:23<01:30,  1.71it/s, v_num=56da]\n",
            "Epoch 1:  31%|███       | 60/195 [00:35<01:19,  1.69it/s, v_num=56da]\n",
            "Epoch 1:  31%|███       | 60/195 [00:35<01:19,  1.69it/s, v_num=56da]\n",
            "Epoch 1:  41%|████      | 80/195 [00:48<01:09,  1.67it/s, v_num=56da]\n",
            "Epoch 1:  41%|████      | 80/195 [00:48<01:09,  1.67it/s, v_num=56da]\n",
            "Epoch 1:  51%|█████▏    | 100/195 [00:57<00:54,  1.73it/s, v_num=56da]\n",
            "Epoch 1:  51%|█████▏    | 100/195 [00:57<00:54,  1.73it/s, v_num=56da]\n",
            "Epoch 1:  62%|██████▏   | 120/195 [01:10<00:43,  1.71it/s, v_num=56da]\n",
            "Epoch 1:  62%|██████▏   | 120/195 [01:10<00:43,  1.71it/s, v_num=56da]\n",
            "Epoch 1:  72%|███████▏  | 140/195 [01:23<00:32,  1.68it/s, v_num=56da]\n",
            "Epoch 1:  72%|███████▏  | 140/195 [01:23<00:32,  1.68it/s, v_num=56da]\n",
            "Epoch 1:  82%|████████▏ | 160/195 [01:33<00:20,  1.71it/s, v_num=56da]\n",
            "Epoch 1:  82%|████████▏ | 160/195 [01:33<00:20,  1.71it/s, v_num=56da]\n",
            "Epoch 1:  92%|█████████▏| 180/195 [01:45<00:08,  1.71it/s, v_num=56da]\n",
            "Epoch 1:  92%|█████████▏| 180/195 [01:45<00:08,  1.71it/s, v_num=56da]\n",
            "Epoch 1: 100%|██████████| 195/195 [01:54<00:00,  1.71it/s, v_num=56da]\n",
            "Epoch 1: 100%|██████████| 195/195 [01:54<00:00,  1.71it/s, v_num=56da]\n",
            "Epoch 1: 100%|██████████| 195/195 [01:55<00:00,  1.69it/s, v_num=56da]\n",
            "Epoch 1:   0%|          | 0/195 [00:00<?, ?it/s, v_num=56da]          \n",
            "Epoch 2:   0%|          | 0/195 [00:00<?, ?it/s, v_num=56da]\n",
            "Epoch 2:  10%|█         | 20/195 [00:11<01:42,  1.70it/s, v_num=56da]\n",
            "Epoch 2:  10%|█         | 20/195 [00:11<01:42,  1.70it/s, v_num=56da]\n",
            "Epoch 2:  21%|██        | 40/195 [00:23<01:31,  1.70it/s, v_num=56da]\n",
            "Epoch 2:  21%|██        | 40/195 [00:23<01:31,  1.70it/s, v_num=56da]\n",
            "Epoch 2:  31%|███       | 60/195 [00:35<01:19,  1.69it/s, v_num=56da]\n",
            "Epoch 2:  31%|███       | 60/195 [00:35<01:19,  1.69it/s, v_num=56da]\n",
            "Epoch 2:  41%|████      | 80/195 [00:47<01:08,  1.67it/s, v_num=56da]\n",
            "Epoch 2:  41%|████      | 80/195 [00:47<01:08,  1.67it/s, v_num=56da]\n",
            "Epoch 2:  51%|█████▏    | 100/195 [00:58<00:55,  1.71it/s, v_num=56da]\n",
            "Epoch 2:  51%|█████▏    | 100/195 [00:58<00:55,  1.71it/s, v_num=56da]\n",
            "Epoch 2:  62%|██████▏   | 120/195 [01:11<00:44,  1.69it/s, v_num=56da]\n",
            "Epoch 2:  62%|██████▏   | 120/195 [01:11<00:44,  1.69it/s, v_num=56da]\n",
            "Epoch 2:  72%|███████▏  | 140/195 [01:23<00:32,  1.67it/s, v_num=56da]\n",
            "Epoch 2:  72%|███████▏  | 140/195 [01:23<00:32,  1.67it/s, v_num=56da]\n",
            "Epoch 2:  82%|████████▏ | 160/195 [01:33<00:20,  1.71it/s, v_num=56da]\n",
            "Epoch 2:  82%|████████▏ | 160/195 [01:33<00:20,  1.71it/s, v_num=56da]\n",
            "Epoch 2:  92%|█████████▏| 180/195 [01:45<00:08,  1.70it/s, v_num=56da]\n",
            "Epoch 2:  92%|█████████▏| 180/195 [01:45<00:08,  1.70it/s, v_num=56da]\n",
            "Epoch 2: 100%|██████████| 195/195 [01:52<00:00,  1.74it/s, v_num=56da]\n",
            "Epoch 2: 100%|██████████| 195/195 [01:52<00:00,  1.74it/s, v_num=56da]\n",
            "Epoch 2: 100%|██████████| 195/195 [01:53<00:00,  1.73it/s, v_num=56da]\n",
            "Epoch 2:   0%|          | 0/195 [00:00<?, ?it/s, v_num=56da]          \n",
            "Epoch 3:   0%|          | 0/195 [00:00<?, ?it/s, v_num=56da]\n",
            "Epoch 3:  10%|█         | 20/195 [00:11<01:43,  1.68it/s, v_num=56da]\n",
            "Epoch 3:  10%|█         | 20/195 [00:11<01:44,  1.68it/s, v_num=56da]\n",
            "Epoch 3:  21%|██        | 40/195 [00:24<01:33,  1.65it/s, v_num=56da]\n",
            "Epoch 3:  21%|██        | 40/195 [00:24<01:33,  1.65it/s, v_num=56da]\n",
            "Epoch 3:  31%|███       | 60/195 [00:36<01:22,  1.64it/s, v_num=56da]\n",
            "Epoch 3:  31%|███       | 60/195 [00:36<01:22,  1.63it/s, v_num=56da]\n",
            "Epoch 3:  41%|████      | 80/195 [00:49<01:11,  1.62it/s, v_num=56da]\n",
            "Epoch 3:  41%|████      | 80/195 [00:49<01:11,  1.62it/s, v_num=56da]\n",
            "Epoch 3:  51%|█████▏    | 100/195 [01:00<00:57,  1.66it/s, v_num=56da]\n",
            "Epoch 3:  51%|█████▏    | 100/195 [01:00<00:57,  1.66it/s, v_num=56da]\n",
            "Epoch 3:  62%|██████▏   | 120/195 [01:12<00:45,  1.65it/s, v_num=56da]\n",
            "Epoch 3:  62%|██████▏   | 120/195 [01:12<00:45,  1.65it/s, v_num=56da]\n",
            "Epoch 3:  72%|███████▏  | 140/195 [01:25<00:33,  1.64it/s, v_num=56da]\n",
            "Epoch 3:  72%|███████▏  | 140/195 [01:25<00:33,  1.64it/s, v_num=56da]\n",
            "Epoch 3:  82%|████████▏ | 160/195 [01:35<00:20,  1.68it/s, v_num=56da]\n",
            "Epoch 3:  82%|████████▏ | 160/195 [01:35<00:20,  1.68it/s, v_num=56da]\n",
            "Epoch 3:  92%|█████████▏| 180/195 [01:48<00:09,  1.67it/s, v_num=56da]\n",
            "Epoch 3:  92%|█████████▏| 180/195 [01:48<00:09,  1.67it/s, v_num=56da]\n",
            "Epoch 3: 100%|██████████| 195/195 [01:55<00:00,  1.68it/s, v_num=56da]\n",
            "Epoch 3: 100%|██████████| 195/195 [01:55<00:00,  1.68it/s, v_num=56da]\n",
            "Epoch 3: 100%|██████████| 195/195 [01:56<00:00,  1.67it/s, v_num=56da]\n",
            "Epoch 3:   0%|          | 0/195 [00:00<?, ?it/s, v_num=56da]          \n",
            "Epoch 4:   0%|          | 0/195 [00:00<?, ?it/s, v_num=56da]\n",
            "Epoch 4:  10%|█         | 20/195 [00:12<01:46,  1.65it/s, v_num=56da]\n",
            "Epoch 4:  10%|█         | 20/195 [00:12<01:46,  1.64it/s, v_num=56da]\n",
            "Epoch 4:  21%|██        | 40/195 [00:23<01:31,  1.70it/s, v_num=56da]\n",
            "Epoch 4:  21%|██        | 40/195 [00:23<01:31,  1.70it/s, v_num=56da]\n",
            "Epoch 4:  31%|███       | 60/195 [00:37<01:23,  1.62it/s, v_num=56da]\n",
            "Epoch 4:  31%|███       | 60/195 [00:37<01:23,  1.62it/s, v_num=56da]\n",
            "Epoch 4:  41%|████      | 80/195 [00:48<01:10,  1.64it/s, v_num=56da]\n",
            "Epoch 4:  41%|████      | 80/195 [00:48<01:10,  1.64it/s, v_num=56da]\n",
            "Epoch 4:  51%|█████▏    | 100/195 [00:59<00:56,  1.68it/s, v_num=56da]\n",
            "Epoch 4:  51%|█████▏    | 100/195 [00:59<00:56,  1.68it/s, v_num=56da]\n",
            "Epoch 4:  62%|██████▏   | 120/195 [01:12<00:45,  1.65it/s, v_num=56da]\n",
            "Epoch 4:  62%|██████▏   | 120/195 [01:12<00:45,  1.65it/s, v_num=56da]\n",
            "Epoch 4:  72%|███████▏  | 140/195 [01:24<00:33,  1.65it/s, v_num=56da]\n",
            "Epoch 4:  72%|███████▏  | 140/195 [01:24<00:33,  1.65it/s, v_num=56da]\n",
            "Epoch 4:  82%|████████▏ | 160/195 [01:34<00:20,  1.69it/s, v_num=56da]\n",
            "Epoch 4:  82%|████████▏ | 160/195 [01:34<00:20,  1.69it/s, v_num=56da]\n",
            "Epoch 4:  92%|█████████▏| 180/195 [01:47<00:08,  1.68it/s, v_num=56da]\n",
            "Epoch 4:  92%|█████████▏| 180/195 [01:47<00:08,  1.68it/s, v_num=56da]\n",
            "Epoch 4: 100%|██████████| 195/195 [01:55<00:00,  1.69it/s, v_num=56da]\n",
            "Epoch 4: 100%|██████████| 195/195 [01:55<00:00,  1.69it/s, v_num=56da]\n",
            "Epoch 4: 100%|██████████| 195/195 [01:56<00:00,  1.68it/s, v_num=56da]\n",
            "Epoch 4: 100%|██████████| 195/195 [01:56<00:00,  1.68it/s, v_num=56da]\n",
            "/content/solo-learn/solo/backbones/convnext/convnext.py:27: UserWarning: Overwriting convnext_tiny in registry with solo.backbones.convnext.convnext.convnext_tiny. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def convnext_tiny(**kwargs):\n",
            "/content/solo-learn/solo/backbones/convnext/convnext.py:34: UserWarning: Overwriting convnext_small in registry with solo.backbones.convnext.convnext.convnext_small. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def convnext_small(**kwargs):\n",
            "/content/solo-learn/solo/backbones/convnext/convnext.py:41: UserWarning: Overwriting convnext_base in registry with solo.backbones.convnext.convnext.convnext_base. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def convnext_base(**kwargs):\n",
            "/content/solo-learn/solo/backbones/convnext/convnext.py:48: UserWarning: Overwriting convnext_large in registry with solo.backbones.convnext.convnext.convnext_large. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def convnext_large(**kwargs):\n",
            "/content/solo-learn/solo/backbones/poolformer/poolformer.py:422: UserWarning: Overwriting poolformer_s12 in registry with solo.backbones.poolformer.poolformer.poolformer_s12. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def poolformer_s12(**kwargs):\n",
            "/content/solo-learn/solo/backbones/poolformer/poolformer.py:447: UserWarning: Overwriting poolformer_s24 in registry with solo.backbones.poolformer.poolformer.poolformer_s24. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def poolformer_s24(**kwargs):\n",
            "/content/solo-learn/solo/backbones/poolformer/poolformer.py:468: UserWarning: Overwriting poolformer_s36 in registry with solo.backbones.poolformer.poolformer.poolformer_s36. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def poolformer_s36(**kwargs):\n",
            "/content/solo-learn/solo/backbones/poolformer/poolformer.py:490: UserWarning: Overwriting poolformer_m36 in registry with solo.backbones.poolformer.poolformer.poolformer_m36. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def poolformer_m36(**kwargs):\n",
            "/content/solo-learn/solo/backbones/poolformer/poolformer.py:512: UserWarning: Overwriting poolformer_m48 in registry with solo.backbones.poolformer.poolformer.poolformer_m48. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def poolformer_m48(**kwargs):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "wandb: Tracking run with wandb version 0.17.2\n",
            "wandb: W&B syncing is set to `offline` in this directory.  \n",
            "wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.\n",
            "wandb: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
            "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "wandb: \n",
            "wandb: Run history:\n",
            "wandb:        dino_loss_epoch █▂▁▁▁\n",
            "wandb:         dino_loss_step █▇▇▇▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "wandb:                  epoch ▁▁▁▁▃▃▃▃▃▅▅▅▅▅▆▆▆▆▆█████\n",
            "wandb:       lr-LARS/backbone ▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██\n",
            "wandb:     lr-LARS/classifier ▁▁▂▂▃▃▃▄▄▄▅▅▆▆▆▇▇██\n",
            "wandb:           lr-LARS/head ▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██\n",
            "wandb:                    tau ▁▁▁▂▂▂▃▃▄▅▅▆▆▇▇▇███\n",
            "wandb:       train_acc1_epoch ▁▅▇▇█\n",
            "wandb:        train_acc1_step ▁▃▄▄▃▆▆▇▇▆█▇▅▇▄▆▆▇█\n",
            "wandb:       train_acc5_epoch ▁▆▇██\n",
            "wandb:        train_acc5_step ▁▄▆▇▆▇▇▆█▇▇▆▆▇▇▇█▇█\n",
            "wandb: train_class_loss_epoch █▃▁▂▂\n",
            "wandb:  train_class_loss_step █▇▄▄▅▃▄▄▁▃▃▄▃▃▅▄▄▃▂\n",
            "wandb:    trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██\n",
            "wandb: \n",
            "wandb: Run summary:\n",
            "wandb:        dino_loss_epoch 8.31128\n",
            "wandb:         dino_loss_step 8.31214\n",
            "wandb:                  epoch 4\n",
            "wandb:       lr-LARS/backbone 0.14609\n",
            "wandb:     lr-LARS/classifier 0.04871\n",
            "wandb:           lr-LARS/head 0.14609\n",
            "wandb:                    tau 1.0\n",
            "wandb:       train_acc1_epoch 23.15605\n",
            "wandb:        train_acc1_step 25.58594\n",
            "wandb:       train_acc5_epoch 74.18469\n",
            "wandb:        train_acc5_step 75.78125\n",
            "wandb: train_class_loss_epoch 2.13522\n",
            "wandb:  train_class_loss_step 2.07494\n",
            "wandb:    trainer/global_step 974\n",
            "wandb: \n",
            "wandb: You can sync this run to the cloud by running:\n",
            "wandb: wandb sync ./wandb/offline-run-20240622_223249-5m5e56da\n",
            "wandb: Find logs at: ./wandb/offline-run-20240622_223249-5m5e56da/logs\n",
            "wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### evaluate the model"
      ],
      "metadata": {
        "id": "PUYFIk1lCz-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for dirpath, _, filenames in os.walk(os.path.join(os.getcwd(), 'trained_models', 'dino')):\n",
        "  if filenames:\n",
        "    assert filenames[1].endswith('ckpt')\n",
        "    new_path = os.path.join(dirpath, filenames[1].replace('=', '_'))\n",
        "    os.rename(os.path.join(dirpath, filenames[1]), new_path)"
      ],
      "metadata": {
        "id": "H9QOKgqn0BAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "command_path_name = '/'.join(new_path.split('/')[2:])"
      ],
      "metadata": {
        "id": "BnNzxqGD0O50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "command = ['python3', 'solo-learn/main_linear.py',\n",
        "           '--config-path', 'scripts/linear/cifar/',\n",
        "           '--config-name', 'dino.yaml',\n",
        "           f'++pretrained_feature_extractor={command_path_name}',\n",
        "           '++wandb.offline=True']"
      ],
      "metadata": {
        "id": "D-nm6evf0OyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "\n",
        "for line in process.stdout:\n",
        "    print(line, end='')\n",
        "\n",
        "for line in process.stderr:\n",
        "    print(line, end='')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrkFLbv-0zld",
        "outputId": "0335c15a-1d06-4664-d276-7054ffba7d17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Sanity Checking: |          | 0/? [00:00<?, ?it/s]\n",
            "Sanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  3.17it/s]\n",
            "                                                                           \n",
            "\n",
            "Training: |          | 0/? [00:00<?, ?it/s]\n",
            "Training:   0%|          | 0/195 [00:00<?, ?it/s]\n",
            "Epoch 0:   0%|          | 0/195 [00:00<?, ?it/s] \n",
            "Epoch 0:  10%|█         | 20/195 [00:02<00:20,  8.44it/s]\n",
            "Epoch 0:  10%|█         | 20/195 [00:02<00:20,  8.42it/s, v_num=2ekp]\n",
            "Epoch 0:  21%|██        | 40/195 [00:04<00:18,  8.25it/s, v_num=2ekp]\n",
            "Epoch 0:  21%|██        | 40/195 [00:04<00:18,  8.24it/s, v_num=2ekp]\n",
            "Epoch 0:  31%|███       | 60/195 [00:08<00:18,  7.20it/s, v_num=2ekp]\n",
            "Epoch 0:  31%|███       | 60/195 [00:08<00:18,  7.19it/s, v_num=2ekp]\n",
            "Epoch 0:  41%|████      | 80/195 [00:12<00:17,  6.58it/s, v_num=2ekp]\n",
            "Epoch 0:  41%|████      | 80/195 [00:12<00:17,  6.58it/s, v_num=2ekp]\n",
            "Epoch 0:  51%|█████▏    | 100/195 [00:15<00:14,  6.62it/s, v_num=2ekp]\n",
            "Epoch 0:  51%|█████▏    | 100/195 [00:15<00:14,  6.62it/s, v_num=2ekp]\n",
            "Epoch 0:  62%|██████▏   | 120/195 [00:16<00:10,  7.13it/s, v_num=2ekp]\n",
            "Epoch 0:  62%|██████▏   | 120/195 [00:16<00:10,  7.13it/s, v_num=2ekp]\n",
            "Epoch 0:  72%|███████▏  | 140/195 [00:19<00:07,  7.32it/s, v_num=2ekp]\n",
            "Epoch 0:  72%|███████▏  | 140/195 [00:19<00:07,  7.32it/s, v_num=2ekp]\n",
            "Epoch 0:  82%|████████▏ | 160/195 [00:21<00:04,  7.52it/s, v_num=2ekp]\n",
            "Epoch 0:  82%|████████▏ | 160/195 [00:21<00:04,  7.52it/s, v_num=2ekp]\n",
            "Epoch 0:  92%|█████████▏| 180/195 [00:23<00:01,  7.73it/s, v_num=2ekp]\n",
            "Epoch 0:  92%|█████████▏| 180/195 [00:23<00:01,  7.73it/s, v_num=2ekp]\n",
            "Epoch 0: 100%|██████████| 195/195 [00:25<00:00,  7.69it/s, v_num=2ekp]\n",
            "Epoch 0: 100%|██████████| 195/195 [00:25<00:00,  7.69it/s, v_num=2ekp]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:  50%|█████     | 20/40 [00:01<00:01, 10.05it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 40/40 [00:03<00:00, 10.17it/s]\u001b[A\n",
            "\n",
            "                                                                        \u001b[A\n",
            "Epoch 0: 100%|██████████| 195/195 [00:29<00:00,  6.51it/s, v_num=2ekp]\n",
            "Epoch 0: 100%|██████████| 195/195 [00:30<00:00,  6.46it/s, v_num=2ekp]\n",
            "Epoch 0:   0%|          | 0/195 [00:00<?, ?it/s, v_num=2ekp]          \n",
            "Epoch 1:   0%|          | 0/195 [00:00<?, ?it/s, v_num=2ekp]\n",
            "Epoch 1:  10%|█         | 20/195 [00:04<00:36,  4.79it/s, v_num=2ekp]\n",
            "Epoch 1:  10%|█         | 20/195 [00:04<00:36,  4.79it/s, v_num=2ekp]\n",
            "Epoch 1:  21%|██        | 40/195 [00:06<00:24,  6.20it/s, v_num=2ekp]\n",
            "Epoch 1:  21%|██        | 40/195 [00:06<00:24,  6.20it/s, v_num=2ekp]\n",
            "Epoch 1:  31%|███       | 60/195 [00:08<00:19,  6.93it/s, v_num=2ekp]\n",
            "Epoch 1:  31%|███       | 60/195 [00:08<00:19,  6.93it/s, v_num=2ekp]\n",
            "Epoch 1:  41%|████      | 80/195 [00:10<00:15,  7.39it/s, v_num=2ekp]\n",
            "Epoch 1:  41%|████      | 80/195 [00:10<00:15,  7.38it/s, v_num=2ekp]\n",
            "Epoch 1:  51%|█████▏    | 100/195 [00:12<00:12,  7.74it/s, v_num=2ekp]\n",
            "Epoch 1:  51%|█████▏    | 100/195 [00:12<00:12,  7.74it/s, v_num=2ekp]\n",
            "Epoch 1:  62%|██████▏   | 120/195 [00:15<00:09,  7.51it/s, v_num=2ekp]\n",
            "Epoch 1:  62%|██████▏   | 120/195 [00:15<00:09,  7.51it/s, v_num=2ekp]\n",
            "Epoch 1:  72%|███████▏  | 140/195 [00:19<00:07,  7.09it/s, v_num=2ekp]\n",
            "Epoch 1:  72%|███████▏  | 140/195 [00:19<00:07,  7.09it/s, v_num=2ekp]\n",
            "Epoch 1:  82%|████████▏ | 160/195 [00:23<00:05,  6.83it/s, v_num=2ekp]\n",
            "Epoch 1:  82%|████████▏ | 160/195 [00:23<00:05,  6.83it/s, v_num=2ekp]\n",
            "Epoch 1:  92%|█████████▏| 180/195 [00:25<00:02,  7.06it/s, v_num=2ekp]\n",
            "Epoch 1:  92%|█████████▏| 180/195 [00:25<00:02,  7.06it/s, v_num=2ekp]\n",
            "Epoch 1: 100%|██████████| 195/195 [00:27<00:00,  7.19it/s, v_num=2ekp]\n",
            "Epoch 1: 100%|██████████| 195/195 [00:27<00:00,  7.19it/s, v_num=2ekp]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:  50%|█████     | 20/40 [00:01<00:01, 18.23it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 40/40 [00:02<00:00, 19.36it/s]\u001b[A\n",
            "\n",
            "                                                                        \u001b[A\n",
            "Epoch 1: 100%|██████████| 195/195 [00:29<00:00,  6.58it/s, v_num=2ekp]\n",
            "Epoch 1: 100%|██████████| 195/195 [00:29<00:00,  6.54it/s, v_num=2ekp]\n",
            "Epoch 1:   0%|          | 0/195 [00:00<?, ?it/s, v_num=2ekp]          \n",
            "Epoch 2:   0%|          | 0/195 [00:00<?, ?it/s, v_num=2ekp]\n",
            "Epoch 2:  10%|█         | 20/195 [00:02<00:22,  7.70it/s, v_num=2ekp]\n",
            "Epoch 2:  10%|█         | 20/195 [00:02<00:22,  7.69it/s, v_num=2ekp]\n",
            "Epoch 2:  21%|██        | 40/195 [00:05<00:22,  6.98it/s, v_num=2ekp]\n",
            "Epoch 2:  21%|██        | 40/195 [00:05<00:22,  6.98it/s, v_num=2ekp]\n",
            "Epoch 2:  31%|███       | 60/195 [00:09<00:21,  6.33it/s, v_num=2ekp]\n",
            "Epoch 2:  31%|███       | 60/195 [00:09<00:21,  6.32it/s, v_num=2ekp]\n",
            "Epoch 2:  41%|████      | 80/195 [00:13<00:19,  6.04it/s, v_num=2ekp]\n",
            "Epoch 2:  41%|████      | 80/195 [00:13<00:19,  6.03it/s, v_num=2ekp]\n",
            "Epoch 2:  51%|█████▏    | 100/195 [00:15<00:14,  6.46it/s, v_num=2ekp]\n",
            "Epoch 2:  51%|█████▏    | 100/195 [00:15<00:14,  6.46it/s, v_num=2ekp]\n",
            "Epoch 2:  62%|██████▏   | 120/195 [00:17<00:11,  6.81it/s, v_num=2ekp]\n",
            "Epoch 2:  62%|██████▏   | 120/195 [00:17<00:11,  6.80it/s, v_num=2ekp]\n",
            "Epoch 2:  72%|███████▏  | 140/195 [00:19<00:07,  7.07it/s, v_num=2ekp]\n",
            "Epoch 2:  72%|███████▏  | 140/195 [00:19<00:07,  7.07it/s, v_num=2ekp]\n",
            "Epoch 2:  82%|████████▏ | 160/195 [00:22<00:04,  7.27it/s, v_num=2ekp]\n",
            "Epoch 2:  82%|████████▏ | 160/195 [00:22<00:04,  7.27it/s, v_num=2ekp]\n",
            "Epoch 2:  92%|█████████▏| 180/195 [00:24<00:02,  7.30it/s, v_num=2ekp]\n",
            "Epoch 2:  92%|█████████▏| 180/195 [00:24<00:02,  7.30it/s, v_num=2ekp]\n",
            "Epoch 2: 100%|██████████| 195/195 [00:27<00:00,  7.22it/s, v_num=2ekp]\n",
            "Epoch 2: 100%|██████████| 195/195 [00:27<00:00,  7.22it/s, v_num=2ekp]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:  50%|█████     | 20/40 [00:01<00:01, 10.36it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 40/40 [00:03<00:00, 13.24it/s]\u001b[A\n",
            "\n",
            "                                                                        \u001b[A\n",
            "Epoch 2: 100%|██████████| 195/195 [00:31<00:00,  6.29it/s, v_num=2ekp]\n",
            "Epoch 2: 100%|██████████| 195/195 [00:31<00:00,  6.26it/s, v_num=2ekp]\n",
            "Epoch 2:   0%|          | 0/195 [00:00<?, ?it/s, v_num=2ekp]          \n",
            "Epoch 3:   0%|          | 0/195 [00:00<?, ?it/s, v_num=2ekp]\n",
            "Epoch 3:  10%|█         | 20/195 [00:02<00:22,  7.73it/s, v_num=2ekp]\n",
            "Epoch 3:  10%|█         | 20/195 [00:02<00:22,  7.71it/s, v_num=2ekp]\n",
            "Epoch 3:  21%|██        | 40/195 [00:04<00:18,  8.36it/s, v_num=2ekp]\n",
            "Epoch 3:  21%|██        | 40/195 [00:04<00:18,  8.36it/s, v_num=2ekp]\n",
            "Epoch 3:  31%|███       | 60/195 [00:07<00:15,  8.51it/s, v_num=2ekp]\n",
            "Epoch 3:  31%|███       | 60/195 [00:07<00:15,  8.51it/s, v_num=2ekp]\n",
            "Epoch 3:  41%|████      | 80/195 [00:09<00:13,  8.60it/s, v_num=2ekp]\n",
            "Epoch 3:  41%|████      | 80/195 [00:09<00:13,  8.59it/s, v_num=2ekp]\n",
            "Epoch 3:  51%|█████▏    | 100/195 [00:12<00:12,  7.85it/s, v_num=2ekp]\n",
            "Epoch 3:  51%|█████▏    | 100/195 [00:12<00:12,  7.85it/s, v_num=2ekp]\n",
            "Epoch 3:  62%|██████▏   | 120/195 [00:16<00:10,  7.15it/s, v_num=2ekp]\n",
            "Epoch 3:  62%|██████▏   | 120/195 [00:16<00:10,  7.15it/s, v_num=2ekp]\n",
            "Epoch 3:  72%|███████▏  | 140/195 [00:19<00:07,  7.01it/s, v_num=2ekp]\n",
            "Epoch 3:  72%|███████▏  | 140/195 [00:19<00:07,  7.01it/s, v_num=2ekp]\n",
            "Epoch 3:  82%|████████▏ | 160/195 [00:22<00:04,  7.27it/s, v_num=2ekp]\n",
            "Epoch 3:  82%|████████▏ | 160/195 [00:22<00:04,  7.27it/s, v_num=2ekp]\n",
            "Epoch 3:  92%|█████████▏| 180/195 [00:24<00:02,  7.43it/s, v_num=2ekp]\n",
            "Epoch 3:  92%|█████████▏| 180/195 [00:24<00:02,  7.43it/s, v_num=2ekp]\n",
            "Epoch 3: 100%|██████████| 195/195 [00:25<00:00,  7.62it/s, v_num=2ekp]\n",
            "Epoch 3: 100%|██████████| 195/195 [00:25<00:00,  7.62it/s, v_num=2ekp]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:  50%|█████     | 20/40 [00:01<00:01, 18.58it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 40/40 [00:02<00:00, 17.58it/s]\u001b[A\n",
            "\n",
            "                                                                        \u001b[A\n",
            "Epoch 3: 100%|██████████| 195/195 [00:28<00:00,  6.88it/s, v_num=2ekp]\n",
            "Epoch 3: 100%|██████████| 195/195 [00:28<00:00,  6.84it/s, v_num=2ekp]\n",
            "Epoch 3:   0%|          | 0/195 [00:00<?, ?it/s, v_num=2ekp]          \n",
            "Epoch 4:   0%|          | 0/195 [00:00<?, ?it/s, v_num=2ekp]\n",
            "Epoch 4:  10%|█         | 20/195 [00:03<00:33,  5.20it/s, v_num=2ekp]\n",
            "Epoch 4:  10%|█         | 20/195 [00:03<00:33,  5.18it/s, v_num=2ekp]\n",
            "Epoch 4:  21%|██        | 40/195 [00:07<00:30,  5.10it/s, v_num=2ekp]\n",
            "Epoch 4:  21%|██        | 40/195 [00:07<00:30,  5.10it/s, v_num=2ekp]\n",
            "Epoch 4:  31%|███       | 60/195 [00:11<00:25,  5.30it/s, v_num=2ekp]\n",
            "Epoch 4:  31%|███       | 60/195 [00:11<00:25,  5.30it/s, v_num=2ekp]\n",
            "Epoch 4:  41%|████      | 80/195 [00:13<00:19,  5.87it/s, v_num=2ekp]\n",
            "Epoch 4:  41%|████      | 80/195 [00:13<00:19,  5.87it/s, v_num=2ekp]\n",
            "Epoch 4:  51%|█████▏    | 100/195 [00:15<00:15,  6.28it/s, v_num=2ekp]\n",
            "Epoch 4:  51%|█████▏    | 100/195 [00:15<00:15,  6.28it/s, v_num=2ekp]\n",
            "Epoch 4:  62%|██████▏   | 120/195 [00:18<00:11,  6.63it/s, v_num=2ekp]\n",
            "Epoch 4:  62%|██████▏   | 120/195 [00:18<00:11,  6.63it/s, v_num=2ekp]\n",
            "Epoch 4:  72%|███████▏  | 140/195 [00:20<00:07,  6.97it/s, v_num=2ekp]\n",
            "Epoch 4:  72%|███████▏  | 140/195 [00:20<00:07,  6.97it/s, v_num=2ekp]\n",
            "Epoch 4:  82%|████████▏ | 160/195 [00:22<00:05,  6.98it/s, v_num=2ekp]\n",
            "Epoch 4:  82%|████████▏ | 160/195 [00:22<00:05,  6.98it/s, v_num=2ekp]\n",
            "Epoch 4:  92%|█████████▏| 180/195 [00:26<00:02,  6.70it/s, v_num=2ekp]\n",
            "Epoch 4:  92%|█████████▏| 180/195 [00:26<00:02,  6.70it/s, v_num=2ekp]\n",
            "Epoch 4: 100%|██████████| 195/195 [00:29<00:00,  6.64it/s, v_num=2ekp]\n",
            "Epoch 4: 100%|██████████| 195/195 [00:29<00:00,  6.64it/s, v_num=2ekp]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:  50%|█████     | 20/40 [00:01<00:01, 13.05it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 40/40 [00:02<00:00, 15.69it/s]\u001b[A\n",
            "\n",
            "                                                                        \u001b[A\n",
            "Epoch 4: 100%|██████████| 195/195 [00:32<00:00,  5.96it/s, v_num=2ekp]\n",
            "Epoch 4: 100%|██████████| 195/195 [00:32<00:00,  5.93it/s, v_num=2ekp]\n",
            "Epoch 4: 100%|██████████| 195/195 [00:32<00:00,  5.93it/s, v_num=2ekp]\n",
            "/content/solo-learn/solo/backbones/convnext/convnext.py:27: UserWarning: Overwriting convnext_tiny in registry with solo.backbones.convnext.convnext.convnext_tiny. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def convnext_tiny(**kwargs):\n",
            "/content/solo-learn/solo/backbones/convnext/convnext.py:34: UserWarning: Overwriting convnext_small in registry with solo.backbones.convnext.convnext.convnext_small. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def convnext_small(**kwargs):\n",
            "/content/solo-learn/solo/backbones/convnext/convnext.py:41: UserWarning: Overwriting convnext_base in registry with solo.backbones.convnext.convnext.convnext_base. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def convnext_base(**kwargs):\n",
            "/content/solo-learn/solo/backbones/convnext/convnext.py:48: UserWarning: Overwriting convnext_large in registry with solo.backbones.convnext.convnext.convnext_large. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def convnext_large(**kwargs):\n",
            "/content/solo-learn/solo/backbones/poolformer/poolformer.py:422: UserWarning: Overwriting poolformer_s12 in registry with solo.backbones.poolformer.poolformer.poolformer_s12. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def poolformer_s12(**kwargs):\n",
            "/content/solo-learn/solo/backbones/poolformer/poolformer.py:447: UserWarning: Overwriting poolformer_s24 in registry with solo.backbones.poolformer.poolformer.poolformer_s24. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def poolformer_s24(**kwargs):\n",
            "/content/solo-learn/solo/backbones/poolformer/poolformer.py:468: UserWarning: Overwriting poolformer_s36 in registry with solo.backbones.poolformer.poolformer.poolformer_s36. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def poolformer_s36(**kwargs):\n",
            "/content/solo-learn/solo/backbones/poolformer/poolformer.py:490: UserWarning: Overwriting poolformer_m36 in registry with solo.backbones.poolformer.poolformer.poolformer_m36. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def poolformer_m36(**kwargs):\n",
            "/content/solo-learn/solo/backbones/poolformer/poolformer.py:512: UserWarning: Overwriting poolformer_m48 in registry with solo.backbones.poolformer.poolformer.poolformer_m48. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  def poolformer_m48(**kwargs):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "wandb: Tracking run with wandb version 0.17.2\n",
            "wandb: W&B syncing is set to `offline` in this directory.  \n",
            "wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.\n",
            "wandb: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
            "/usr/local/lib/python3.10/dist-packages/lightning/fabric/connector.py:565: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "wandb: \n",
            "wandb: Run history:\n",
            "wandb:               epoch ▁▁▁▁▁▃▃▃▃▃▃▅▅▅▅▅▅▆▆▆▆▆▆██████\n",
            "wandb:              lr-SGD ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "wandb:    train_acc1_epoch ▁▅▆▇█\n",
            "wandb:     train_acc1_step ▄▅▄▅▅▅▄▆▁▅██▆▅▅▅▅▅▃\n",
            "wandb:    train_acc5_epoch ▁▅▅▇█\n",
            "wandb:     train_acc5_step ▄▄▄▄▅█▂▅▄▁▆█▅▄▅▇▂▇▄\n",
            "wandb:    train_loss_epoch █▃▃▁▁\n",
            "wandb:     train_loss_step ▆▃▅▃▃▁▅▂▅█▁▁▂▂▃▂▅▂▂\n",
            "wandb: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n",
            "wandb:            val_acc1 ▁█▅▂▇\n",
            "wandb:            val_acc5 ▁▂▄▅█\n",
            "wandb:            val_loss █▂▂▂▁\n",
            "wandb: \n",
            "wandb: Run summary:\n",
            "wandb:               epoch 4\n",
            "wandb:              lr-SGD 0.3\n",
            "wandb:    train_acc1_epoch 22.18349\n",
            "wandb:     train_acc1_step 12.89062\n",
            "wandb:    train_acc5_epoch 68.65184\n",
            "wandb:     train_acc5_step 63.28125\n",
            "wandb:    train_loss_epoch 24.95775\n",
            "wandb:     train_loss_step 21.54278\n",
            "wandb: trainer/global_step 974\n",
            "wandb:            val_acc1 28.16\n",
            "wandb:            val_acc5 75.62\n",
            "wandb:            val_loss 15.60103\n",
            "wandb: \n",
            "wandb: You can sync this run to the cloud by running:\n",
            "wandb: wandb sync ./wandb/offline-run-20240622_224606-424l2ekp\n",
            "wandb: Find logs at: ./wandb/offline-run-20240622_224606-424l2ekp/logs\n",
            "wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jWzq4rq9ox49"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}